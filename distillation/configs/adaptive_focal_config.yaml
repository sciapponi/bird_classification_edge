# Adaptive Focal Distillation Configuration
# Best for highly imbalanced datasets where class distribution changes

defaults:
  - distillation_config

# Override experiment name
experiment_name: "adaptive_focal_distillation"

# Loss function configuration - Adaptive focal distillation
loss:
  type: "focal_distillation"    # Focal loss + knowledge distillation
  gamma: 2.0                    # Starting gamma (will be adapted)
  class_weights: "auto"         # Automatically compute and adapt
  alpha_scaling: 2.0            # Higher scaling for very imbalanced data
  
  # FORCE FULL DATASET CALCULATION FOR PRODUCTION
  use_fast_sampling: false      # Use ALL samples for accurate weights
  weight_calculation_samples: null  # Use all available samples
  cache_max_age_hours: 24       # Cache for 1 day

# Adaptive distillation parameters
distillation:
  alpha: 0.25                   # Start with more focus on focal loss
  temperature: 5.0              # Higher temperature for softer distillation
  adaptive: true                # Enable adaptive alpha adjustment
  adaptation_rate: 0.15         # More aggressive adaptation

# Training parameters optimized for adaptive learning
training:
  epochs: 75                    # Medium-length training
  batch_size: 48                # Slightly smaller batches for better gradient estimates
  patience: 20                  # Patience for adaptive methods
  min_delta: 0.0005             # More sensitive to improvements
  seed: 42

# Model parameters - robust architecture for imbalanced data
model:
  spectrogram_type: "combined_log_linear"
  hidden_dim: 80                # Slightly larger for complex imbalanced patterns
  n_mel_bins: 64
  n_linear_filters: 64
  trainable_filterbank: true
  initial_breakpoint: 4000.0
  initial_transition_width: 100.0
  n_fft: 1024
  hop_length: 320
  matchbox:
    base_filters: 32            # Keep compatible with base config
    num_layers: 3               # Keep compatible with base config  
    kernel_size: 3
    dropout: 0.15               # Higher dropout for regularization

# Optimizer with careful learning rate for adaptive methods
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0008                    # Lower LR for stable adaptive learning
  weight_decay: 0.015
  betas: [0.9, 0.999]
  eps: 1e-8

# More conservative scheduler for adaptive learning (for main model)
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: 'min'
  factor: 0.6                   # Less aggressive reduction
  patience: 7                   # Medium patience
  min_lr: 5e-7
  cooldown: 3                   # Cooldown period after reduction

# Filter scheduler (separate scheduler for filter parameters)
# Using CosineAnnealingLR for filters with separate schedule
filter_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 50                      # Separate annealing schedule for filters  
  eta_min: 5e-7                  # Minimum learning rate for filters
# Alternative: Set to null to disable scheduler for filters (constant LR)
# filter_scheduler: null

# Dataset configuration - full imbalanced dataset
dataset:
  soft_labels_path: "test_soft_labels"
  main_data_dir: bird_sound_dataset
  allowed_bird_classes:
    - "Bubo_bubo"
    - "Certhia_familiaris"
    - "Apus_apus"
    - "Certhia_brachydactyla"
    - "Emberiza_cia"
    - "Lophophanes_cristatus"
    - "Periparus_ater"
    - "Poecile_montanus"
  
  # Use full no_birds dataset to maximize imbalance
  load_pregenerated_no_birds: true
  pregenerated_no_birds_dir: augmented_dataset/no_birds
  num_no_bird_samples: 836      # Use all for maximum imbalance challenge
  
  # Moderate augmentation - let adaptive methods handle the complexity
  augmentation:
    enabled: true
    noise_level: 0.015          # Moderate noise
    time_mask_param: 35
    freq_mask_param: 12
    time_shift_limit: 0.12
    speed_perturb_rate_min: 0.92
    speed_perturb_rate_max: 1.08
  
  sample_rate: 32000
  clip_duration: 3.0
  lowcut: 150.0
  highcut: 16000.0
  extract_calls: true
  
  esc50_dir: "esc-50/ESC-50-master"
  val_split: 0.15
  test_split: 0.15
  seed: 42 