# Configuration for Knowledge Distillation Training
# Standalone configuration with all required parameters

# Override experiment name
experiment_name: "easy_species_full_dataset"

# Loss function configuration
loss:
  type: "focal_distillation"          # Options: "distillation", "focal", "focal_distillation"
  gamma: 4.0                    # Focal loss gamma parameter (only used for focal loss types)
  class_weights: auto           # Options: null, "auto", or list of per-class weights
  alpha_scaling: 3.0            # Scaling factor for automatic class weights
  auto_weights_sample_size: 5000 # NEW: Number of samples to check for auto weights
  adaptive: true
# Distillation-specific parameters
distillation:
  # Distillation loss parameters
  alpha: 0.4                   # Weight for soft labels (0-1, 0=only hard labels, 1=only soft labels)
  temperature: 3.0              # Temperature scaling for softmax (higher=softer distributions)
  adaptive: true               # Whether to use adaptive alpha based on validation performance
  adaptation_rate: 0.1          # Rate of alpha adaptation (if adaptive=true)
  
  # Alpha scheduling (if not adaptive)
  alpha_schedule: "constant"    # Options: "constant", "linear_increase", "cosine"
  
  # Confidence threshold used during soft label extraction
  confidence_threshold: 0.05    # Must match the threshold used in extract_soft_labels.py

# Path to soft labels (generated by extract_soft_labels.py)
# soft_labels_path: "soft_labels_complete"  # MOVED into dataset section

# Training parameters (adjusted for distillation)
training:
  epochs: 150           # AUMENTATO - Alternating richiede più epochs
  batch_size: 64               # Can be larger since we're not running teacher online
  patience: 35                  # AUMENTATO - Patience maggiore per alternating
  min_delta: 0.001             # More sensitive to small improvements
  seed: 42

# Model parameters (keep student model lightweight)
model:
  spectrogram_type: "combined_log_linear"  # Use the best performing architecture
  hidden_dim: 64
  n_mel_bins: 64
  n_linear_filters: 64
  trainable_filterbank: true
  initial_breakpoint: 8000.0 # STRESS TEST: Partenza neutra e alta per forzare una scelta.
  initial_transition_width: 200.0 # STRESS TEST: Valore di partenza neutro.
  n_fft: 512 
  hop_length: 320
  matchbox:
    base_filters: 32
    num_layers: 3
    kernel_size: 3
    dropout: 0.1

# Optimizer (potentially different learning rates for distillation)
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.001                    # Slightly lower LR for distillation stability
  weight_decay: 0.01

# Learning rate scheduler (for main model)
#scheduler:
#  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#  mode: 'min'                   # Monitor validation loss
#  factor: 0.5
#  patience: 5
#  min_lr: 1e-6
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 100                     
  eta_min: 5e-7                 # Minimum learning rate

# Filter scheduler (separate scheduler for filter parameters)
# Using CosineAnnealingLR for filters with separate schedule
filter_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 100                     # Separate annealing schedule for filters
  eta_min: 5e-7                  # Minimum learning rate for filters
# Alternative: Set to null to disable scheduler for filters (constant LR)
# filter_scheduler: null          
# Alternative: Use ReduceLROnPlateau for more conservative updates
# filter_scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   mode: 'min'
#   factor: 0.8                 # More conservative reduction
#   patience: 15                # Longer patience for filter parameters
#   min_lr: 1e-5

# Filter-specific learning rate control (OTTIMIZZATO)
breakpoint_lr_multiplier: 5.0          # STRATEGIA: LR più alto per contrastare la spinta verso il basso del joint training.
transition_width_lr_multiplier: 2.0    # STRATEGIA: LR moderato per la transizione.

# NUOVO: Filter exploration parameters per evitare getting stuck
filter_exploration:
  enable_noise: false                    # STRESS TEST: Disabilitato per osservare l'overshooting puro.
  noise_std: 0.05                     
  enable_oscillation: false              # STRESS TEST: Disabilitato per osservare l'overshooting puro.
  oscillation_amplitude: 25.0          
  oscillation_period: 5                
  enable_momentum_reset: true          
  momentum_reset_period: 10             # STRATEGIA: Reset ogni 10 epochs invece di 5

# NUOVO: Filter regularization per evitare valori estremi
filter_regularization:
  enable_range_penalty: true           # ATTIVATO - Penalizza valori troppo estremi
  sensible_breakpoint_min: 20.0        # RIDOTTO - Breakpoint "sensato" minimo (Hz) - più permissivo
  sensible_breakpoint_max: 32000.0      # Breakpoint "sensato" massimo (Hz)
  range_penalty_weight: 0.01           # AUMENTATO - Peso della penalizzazione (più forte)
  enable_stability_bonus: false         # ATTIVATO - Bonus per stabilità
  stability_window: 5                  # Finestra per calcolare stabilità (epochs)
  stability_bonus_weight: 0.01         # Peso del bonus di stabilità

# NUOVO: Alternating optimization settings
alternating_optimization:
  enabled: true
  main_opt_epochs: 4       # STRATEGIA: Ripristinato a 4 per applicare solo la strategia del LR.
  filter_opt_epochs: 1     # STRATEGIA: 1 epoca di ottimizzazione del filtro ogni 2 di training principale.
  initial_joint_epochs: 5
  final_joint_epochs: 20
  validation_feedback: true
  filter_candidates_test: 5
  search_range_factor: 0.1

# Dataset parameters
dataset:
  soft_labels_path: soft_labels_complete  # Path to soft labels (can be overridden)
  main_data_dir: bird_sound_dataset_processed
  
  # NEW: Control preprocessing behavior
  process: false                         # Set to false to use preprocessed files (no runtime preprocessing)
  preprocessed_root_dir: "bird_sound_dataset_processed"  # Directory with preprocessed files (used when process=false)
  
  # Control whether to include no_bird class
  use_no_bird_class: true               # If true, includes no_bird class; if false, only bird species
  
  # Explicitly define the classes to match the teacher model's output
  allowed_bird_classes:

    - Ardea_cinerea
   - Corvus_corax
    - Falco_tinnunculus
    - Hirundo_rustica
    - Otus_scops
    - Phylloscopus_collybita
    - Picus_canus
    - Strix_aluco
  # no-bird class is auto-added
  load_pregenerated_no_birds: true
  pregenerated_no_birds_dir: augmented_dataset/no_birds
  num_no_bird_samples: 836      # Use all non-bird samples
  
  # Data augmentation (can be less aggressive since we have soft labels)
  augmentation:
    enabled: true
    noise_level: 0.01           # Reduced noise
    time_mask_param: 30
    freq_mask_param: 10
    time_shift_limit: 0.1       # Reduced time shift
    speed_perturb_rate_min: 0.95
    speed_perturb_rate_max: 1.05
  
  # Keep the same preprocessing
  sample_rate: 32000
  clip_duration: 3.0
  lowcut: 150.0
  highcut: 16000.0
  extract_calls: false  # CHANGED: Use call extraction for better training data
  
  # Dynamic balancing for non-bird class
  esc50_dir: "esc-50/ESC-50-master"
  # BALANCING: weighted sampling per gestire imbalance
  balanced_sampling: false     # Abilita weighted sampling
  oversample_rare_classes: 2.0  # Oversampling per classi rare
  num_workers: 0
  pin_memory: true
  val_split: 0.15
  test_split: 0.15
  seed: 42

# Logging and checkpointing
hydra:
  run:
    dir: ./logs/${experiment_name}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    name: ${experiment_name} 