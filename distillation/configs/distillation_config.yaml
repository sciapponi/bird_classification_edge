# Configuration for Knowledge Distillation Training
# Standalone configuration with all required parameters

# Override experiment name
experiment_name: "bird_classification_distillation"

# Distillation-specific parameters
distillation:
  # Distillation loss parameters
  alpha: 0.3                    # Weight for soft labels (0-1, 0=only hard labels, 1=only soft labels)
  temperature: 4.0              # Temperature scaling for softmax (higher=softer distributions)
  adaptive: false               # Whether to use adaptive alpha based on validation performance
  adaptation_rate: 0.1          # Rate of alpha adaptation (if adaptive=true)
  
  # Alpha scheduling (if not adaptive)
  alpha_schedule: "constant"    # Options: "constant", "linear_increase", "cosine"
  
  # Confidence threshold used during soft label extraction
  confidence_threshold: 0.05    # Must match the threshold used in extract_soft_labels.py

# Path to soft labels (generated by extract_soft_labels.py)
# soft_labels_path: "soft_labels_complete"  # MOVED into dataset section

# Training parameters (adjusted for distillation)
training:
  epochs: 30                # More epochs may be beneficial for distillation
  batch_size: 32                # Can be larger since we're not running teacher online
  patience: 15                  # Longer patience for distillation convergence
  min_delta: 0.001
  seed: 42

# Model parameters (keep student model lightweight)
model:
  spectrogram_type: "combined_log_linear"  # Use the best performing architecture
  hidden_dim: 64
  n_mel_bins: 64
  n_linear_filters: 64
  trainable_filterbank: true
  initial_breakpoint: 4000.0
  initial_transition_width: 100.0
  n_fft: 1024
  hop_length: 320
  matchbox:
    base_filters: 32
    num_layers: 3
    kernel_size: 3
    dropout: 0.1

# Optimizer (potentially different learning rates for distillation)
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0005                    # Slightly lower LR for distillation stability
  weight_decay: 0.01

# Learning rate scheduler
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: 'min'                   # Monitor validation loss
  factor: 0.5
  patience: 5
  min_lr: 1e-6

# Dataset parameters
dataset:
  soft_labels_path: "test_soft_labels"  # Path to soft labels (can be overridden)
  main_data_dir: bird_sound_dataset
  # Explicitly define the classes to match the teacher model's output
  allowed_bird_classes:
    - "Bubo_bubo"
    - "Certhia_familiaris"
    - "Apus_apus"
    - "Certhia_brachydactyla"
    - "Emberiza_cia"
    - "Lophophanes_cristatus"
    - "Periparus_ater"
    - "Poecile_montanus"
    # "non-bird" is added automatically
  
  # Include non-bird samples for 5-class classification
  load_pregenerated_no_birds: true
  pregenerated_no_birds_dir: augmented_dataset/no_birds
  num_no_bird_samples: 836      # Use all non-bird samples
  
  # Data augmentation (can be less aggressive since we have soft labels)
  augmentation:
    enabled: true
    noise_level: 0.01           # Reduced noise
    time_mask_param: 30
    freq_mask_param: 10
    time_shift_limit: 0.1       # Reduced time shift
    speed_perturb_rate_min: 0.95
    speed_perturb_rate_max: 1.05
  
  # Keep the same preprocessing
  sample_rate: 32000
  clip_duration: 3.0
  lowcut: 150.0
  highcut: 16000.0
  extract_calls: false
  
  # Dynamic balancing for non-bird class
  esc50_dir: "esc-50/ESC-50-master"
  val_split: 0.15
  test_split: 0.15
  seed: 42

# Logging and checkpointing
hydra:
  run:
    dir: ./logs/${experiment_name}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    name: ${experiment_name} 