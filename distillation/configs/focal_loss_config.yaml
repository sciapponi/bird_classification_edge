# Alternative: Use manual weights to skip calculation entirely
# Copy this section to your config to avoid any weight calculation:
#
# loss:
#   type: "focal_distillation"
#   gamma: 2.0
#   class_weights: [1.5, 1.0, 2.0, 1.0, 0.8]  # Manual weights for [Bubo_bubo, Apus_apus, no_birds, etc.]
#   alpha_scaling: 1.0

# Focal Loss + Knowledge Distillation Configuration
# ULTRA-FAST configuration for rapid testing and development

experiment_name: "focal_loss_ultrafast"

# Loss configuration - Using focal distillation
loss:
  type: "focal_distillation"  # Options: "distillation", "focal", "focal_distillation"
  gamma: 2.0                  # Focal loss gamma parameter (higher = more focus on hard examples)
  class_weights: "auto"       # Options: null, "auto", or list [1.0, 2.0, 1.5] for manual weights
  alpha_scaling: 1.0          # Scaling factor for automatic class weights
  
  # ULTRA-FAST WEIGHT CALCULATION 
  use_fast_sampling: true     # Use statistical sampling instead of full dataset scan
  weight_calculation_samples: 10000  # ULTRA-FAST: Only 100 samples for weight calculation
  cache_max_age_hours: 168    # Cache for 1 week to avoid recalculation

# Standard distillation parameters
distillation:
  alpha: 0.3                  # Weight for soft loss (0.0 = only hard loss, 1.0 = only soft loss)
  temperature: 4.0            # Temperature for softmax in distillation
  adaptive: false             # Whether to use adaptive distillation
  adaptation_rate: 0.1        # Rate for adaptive changes (if adaptive: true)
  confidence_threshold: 0.05  # Must match extract_soft_labels.py

# Training parameters - ULTRA-FAST
training:
  epochs: 1  # Single epoch for ultra-fast testing
  batch_size: 8  # Larger batch for faster processing
  patience: 15
  min_delta: 0.001
  seed: 42
  validation_pause: 2  # Minimal pause
  num_workers: 0  # Single-threaded to avoid deadlocks

# Model parameters
model:
  spectrogram_type: "combined_log_linear"
  hidden_dim: 32  # Smaller model for faster training
  n_mel_bins: 32  # Reduced features
  n_linear_filters: 32
  trainable_filterbank: true
  initial_breakpoint: 4000.0
  initial_transition_width: 100.0
  n_fft: 512      # Smaller FFT for speed
  hop_length: 256 # Larger hop for speed
  matchbox:
    base_filters: 16  # Minimal model size
    num_layers: 2     # Fewer layers
    kernel_size: 3
    dropout: 0.1

# Optimizer
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.002  # Higher learning rate for faster convergence
  weight_decay: 0.01

# Learning rate scheduler
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: 'min'
  factor: 0.5
  patience: 3  # Reduced patience
  min_lr: 1e-6

# Dataset configuration - ULTRA-FAST
dataset:
  soft_labels_path: "test_soft_labels"
  main_data_dir: bird_sound_dataset
  allowed_bird_classes:
    - "Bubo_bubo"
    - "Apus_apus"        # Only 2 classes for ultra-fast testing
  
  load_pregenerated_no_birds: true
  pregenerated_no_birds_dir: augmented_dataset/no_birds
  num_no_bird_samples: 50  # Minimal no-bird samples
  
  # IMPORTANT: Force num_workers to 0 to prevent DataLoader deadlocks
  num_workers: 0
  pin_memory: false  # Disable pin_memory on macOS to reduce issues
  
  # Data augmentation
  augmentation:
    enabled: false  # Disabled for maximum speed
  
  sample_rate: 22050  # Lower sample rate for faster processing
  clip_duration: 2.0  # Shorter clips
  lowcut: 150.0
  highcut: 11025.0    # Nyquist limit for 22050 Hz
  extract_calls: false  # Disabled for speed
  
  esc50_dir: "esc-50/ESC-50-master"
  val_split: 0.15
  test_split: 0.15
  seed: 42

# Debug options - ULTRA-FAST
debug:
  files_limit: 20  # Very limited for ultra-fast testing

# Logging
hydra:
  run:
    dir: ./logs/${experiment_name}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    name: ${experiment_name} 