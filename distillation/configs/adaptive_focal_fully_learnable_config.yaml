# Adaptive Focal Distillation Configuration with Fully Learnable Filter Bank
# Identical to adaptive_focal_config.yaml but tests fully learnable filters

defaults:
  - distillation_config

# Override experiment name
experiment_name: "adaptive_focal_fully_learnable_corrected"

# Loss function configuration - Adaptive focal distillation
loss:
  type: "focal_distillation"    # Focal loss + knowledge distillation
  gamma: 2.0                    # Starting gamma (will be adapted)
  class_weights: "auto"         # Automatically compute and adapt
  alpha_scaling: 2.0            # Higher scaling for very imbalanced data
  num_classes: 9                # EXPLICIT: 8 bird classes + 1 no_birds class
  
  # FORCE FULL DATASET CALCULATION FOR PRODUCTION
  use_fast_sampling: true       # Use sampling for faster weight calculation
  weight_calculation_samples: 10000  # Use 1000 samples for weight calculation
  cache_max_age_hours: 24       # Cache for 1 day

# Adaptive distillation parameters
distillation:
  alpha: 0.25                   # Start with more focus on focal loss
  temperature: 5.0              # Higher temperature for softer distillation
  adaptive: true                # Enable adaptive alpha adjustment
  adaptation_rate: 0.15         # More aggressive adaptation

# Training parameters optimized for adaptive learning
training:
  epochs: 75                    # Medium-length training
  batch_size: 48                # Slightly smaller batches for better gradient estimates
  patience: 40                  # Patience for adaptive methods
  min_delta: 0.0005             # More sensitive to improvements
  seed: 42

# Model parameters - FULLY LEARNABLE FILTER BANK
model:
  num_classes: 9                # EXPLICIT: 8 bird classes + 1 no_birds class
  spectrogram_type: "fully_learnable"  # ‚Üê MAIN CHANGE: Use fully learnable instead of combined_log_linear
  hidden_dim: 32                
  n_mel_bins: 64
  n_linear_filters: 64
  trainable_filterbank: true    # Kept for compatibility but not used in fully learnable
  initial_breakpoint: 4000.0    # Not used in fully learnable, kept for compatibility
  initial_transition_width: 100.0  # Not used in fully learnable, kept for compatibility
  n_fft: 1024
  hop_length: 320
  matchbox:
    base_filters: 32            # Keep compatible with base config
    num_layers: 3               # Keep compatible with base config  
    kernel_size: 3
    dropout: 0.15               # Higher dropout for regularization

# Fully learnable filter specific parameters
filter_init_strategy: "triangular_noise"  # Initialize with triangular + noise for fair comparison
filter_lr_multiplier: 0.1      # Slower learning rate for filter parameters (32k params!)

# Optimizer with careful learning rate for adaptive methods + filter awareness
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0008                    # Lower LR for stable adaptive learning
  weight_decay: 0.015
  betas: [0.9, 0.999]
  eps: 1e-8

# Cosine annealing scheduler for adaptive learning (for main model)
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 75                     # Number of epochs (matches training.epochs)
  eta_min: 5e-7                 # Minimum learning rate

# Filter scheduler (separate scheduler for filter parameters)
# Using CosineAnnealingLR for filters with separate schedule
filter_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 75                      # Separate annealing schedule for filters (matches training.epochs)
  eta_min: 5e-7                  # Minimum learning rate for filters
# Alternative: Set to null to disable scheduler for filters (constant LR)
# filter_scheduler: null          
# Alternative: Use ReduceLROnPlateau for more conservative updates
# filter_scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#   mode: 'min'
#   factor: 0.8                 # More conservative reduction
#   patience: 15                # Longer patience for filter parameters
#   min_lr: 1e-5

# Dataset configuration - SMALL TEST DATASET
dataset:
  soft_labels_path: "test_soft_labels"
  main_data_dir: bird_sound_dataset
  allowed_bird_classes:
    - "Bubo_bubo"
    - "Certhia_familiaris"
    - "Apus_apus"
    - "Certhia_brachydactyla"
    - "Emberiza_cia"
    - "Lophophanes_cristatus"
    - "Periparus_ater"
    - "Poecile_montanus"
  
  # Use minimal no_birds dataset for quick testing
  load_pregenerated_no_birds: true
  pregenerated_no_birds_dir: augmented_dataset/no_birds
  num_no_bird_samples: 836      # Use all no-bird samples for better balance
  
  # Moderate augmentation - let adaptive methods handle the complexity
  augmentation:
    enabled: true
    noise_level: 0.015          # Moderate noise
    time_mask_param: 35
    freq_mask_param: 12
    time_shift_limit: 0.12
    speed_perturb_rate_min: 0.92
    speed_perturb_rate_max: 1.08
  
  sample_rate: 32000
  clip_duration: 3.0
  lowcut: 150.0
  highcut: 16000.0
  extract_calls: true
  
  esc50_dir: "esc-50/ESC-50-master"
  val_split: 0.15
  test_split: 0.15
  seed: 42

# Enhanced monitoring for fully learnable filters
logging:
  monitor_filters: true         # Enable detailed filter monitoring
  filter_visualization_interval: 1   # Save filter plots EVERY epoch for testing
  log_filter_stats: true        # Log filter statistics each epoch
  generate_advanced_plots: true # Force generation of advanced analysis plots 