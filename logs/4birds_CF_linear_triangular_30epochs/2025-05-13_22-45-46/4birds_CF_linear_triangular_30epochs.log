[2025-05-13 22:45:46,177][__main__][INFO] - Experiment: 4birds_CF_linear_triangular_30epochs
[2025-05-13 22:45:46,180][__main__][INFO] - Using device: cpu
[2025-05-13 22:45:46,180][__main__][INFO] - Checking for datasets...
[2025-05-13 22:45:46,180][__main__][INFO] - ESC-50 dataset not found or empty. Downloading...
[2025-05-13 22:45:46,180][__main__][INFO] - Creating initial bird sound datasets (train/val/test)...
[2025-05-13 22:45:46,181][__main__][INFO] - Using dataset split parameters: val=0.15, test=0.15, seed=42
[2025-05-13 22:45:46,221][__main__][INFO] - Initial bird samples: Train=2970, Val=635, Test=635
[2025-05-13 22:45:46,221][__main__][INFO] - Calculating target number of 'no birds' samples...
[2025-05-13 22:45:46,221][__main__][INFO] - Number of bird classes: 4. 'No Birds' label index: 4
[2025-05-13 22:45:46,221][__main__][INFO] - Average samples per bird class in training set: 742.50
[2025-05-13 22:45:46,221][__main__][INFO] - Target 'no birds' samples for training: 742
[2025-05-13 22:45:46,221][__main__][INFO] - Target 'no birds' samples for validation: 159
[2025-05-13 22:45:46,221][__main__][INFO] - Target 'no birds' samples for testing: 159
[2025-05-13 22:45:46,233][__main__][INFO] - Combining bird and 'no birds' datasets...
[2025-05-13 22:45:46,233][__main__][INFO] - Final training samples: 3712
[2025-05-13 22:45:46,233][__main__][INFO] - Final validation samples: 794
[2025-05-13 22:45:46,233][__main__][INFO] - Final testing samples: 794
[2025-05-13 22:45:46,249][__main__][INFO] - Model architecture:
[2025-05-13 22:45:46,249][__main__][INFO] - Improved_Phi_GRU_ATT(
  (stft_transform): Spectrogram()
  (amplitude_to_db): AmplitudeToDB()
  (phi): MatchboxNetSkip(
    (initial_conv_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(4, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(5,), stride=(1,), groups=64, bias=False)
          )
          (5): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(64, 10, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(10, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (blocks_modulelist): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(8, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(4,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (2): ModuleList(
        (0-1): 2 x Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (projections_modulelist): ModuleList(
      (0-2): 3 x Identity()
    )
    (final_block_to_conv1_projection): Identity()
    (final_conv1_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(8, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(5,), stride=(1,), dilation=(2,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (conv1_to_conv2_projection): Identity()
    (final_conv2_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
  )
  (gru): GRU(32, 32, batch_first=True)
  (projection): Linear(in_features=32, out_features=32, bias=True)
  (keyword_attention): AttentionLayer(
    (attention): Linear(in_features=32, out_features=1, bias=True)
  )
  (fc): Linear(in_features=32, out_features=5, bias=True)
)
[2025-05-13 22:45:46,252][__main__][INFO] - Total parameters: 37,478
[2025-05-13 22:45:46,252][__main__][INFO] - Trainable parameters: 37,478
[2025-05-13 22:45:46,252][__main__][INFO] - Computing model complexity...
[2025-05-13 22:46:14,736][__main__][WARNING] - Could not compute MACs: unsupported format string passed to NoneType.__format__
[2025-05-13 22:46:15,400][__main__][INFO] - Starting training...
[2025-05-13 22:51:27,502][__main__][INFO] - Epoch 1/30
[2025-05-13 22:51:27,507][__main__][INFO] - Train Loss: 0.8467, Train Acc: 68.94%
[2025-05-13 22:51:27,507][__main__][INFO] - Val Loss: 0.5449, Val Acc: 80.48%
[2025-05-13 22:51:27,507][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 22:51:27,507][__main__][INFO] - Validation loss improved from inf to 0.5449
[2025-05-13 22:51:27,541][__main__][INFO] - New best model saved with val acc: 80.48%
[2025-05-13 22:56:44,348][__main__][INFO] - Epoch 2/30
[2025-05-13 22:56:44,352][__main__][INFO] - Train Loss: 0.5931, Train Acc: 79.66%
[2025-05-13 22:56:44,352][__main__][INFO] - Val Loss: 0.4887, Val Acc: 84.89%
[2025-05-13 22:56:44,352][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 22:56:44,352][__main__][INFO] - Validation loss improved from 0.5449 to 0.4887
[2025-05-13 22:56:44,402][__main__][INFO] - New best model saved with val acc: 84.89%
[2025-05-13 23:02:29,157][__main__][INFO] - Epoch 3/30
[2025-05-13 23:02:29,161][__main__][INFO] - Train Loss: 0.5194, Train Acc: 82.57%
[2025-05-13 23:02:29,161][__main__][INFO] - Val Loss: 0.4971, Val Acc: 82.49%
[2025-05-13 23:02:29,161][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:02:29,161][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-13 23:07:59,248][__main__][INFO] - Epoch 4/30
[2025-05-13 23:07:59,254][__main__][INFO] - Train Loss: 0.4834, Train Acc: 84.46%
[2025-05-13 23:07:59,254][__main__][INFO] - Val Loss: 0.4620, Val Acc: 83.88%
[2025-05-13 23:07:59,254][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:07:59,254][__main__][INFO] - Validation loss improved from 0.4887 to 0.4620
[2025-05-13 23:13:45,238][__main__][INFO] - Epoch 5/30
[2025-05-13 23:13:45,243][__main__][INFO] - Train Loss: 0.4463, Train Acc: 85.10%
[2025-05-13 23:13:45,243][__main__][INFO] - Val Loss: 0.5574, Val Acc: 80.73%
[2025-05-13 23:13:45,243][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:13:45,243][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-13 23:19:40,306][__main__][INFO] - Epoch 6/30
[2025-05-13 23:19:40,311][__main__][INFO] - Train Loss: 0.4131, Train Acc: 86.53%
[2025-05-13 23:19:40,311][__main__][INFO] - Val Loss: 0.4302, Val Acc: 85.52%
[2025-05-13 23:19:40,311][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:19:40,311][__main__][INFO] - Validation loss improved from 0.4620 to 0.4302
[2025-05-13 23:19:40,338][__main__][INFO] - New best model saved with val acc: 85.52%
[2025-05-13 23:26:01,090][__main__][INFO] - Epoch 7/30
[2025-05-13 23:26:01,094][__main__][INFO] - Train Loss: 0.3978, Train Acc: 86.50%
[2025-05-13 23:26:01,094][__main__][INFO] - Val Loss: 0.6647, Val Acc: 78.59%
[2025-05-13 23:26:01,094][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:26:01,095][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-13 23:31:45,721][__main__][INFO] - Epoch 8/30
[2025-05-13 23:31:45,726][__main__][INFO] - Train Loss: 0.3757, Train Acc: 87.58%
[2025-05-13 23:31:45,726][__main__][INFO] - Val Loss: 0.3977, Val Acc: 87.41%
[2025-05-13 23:31:45,726][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:31:45,726][__main__][INFO] - Validation loss improved from 0.4302 to 0.3977
[2025-05-13 23:31:45,752][__main__][INFO] - New best model saved with val acc: 87.41%
[2025-05-13 23:37:19,399][__main__][INFO] - Epoch 9/30
[2025-05-13 23:37:19,404][__main__][INFO] - Train Loss: 0.3807, Train Acc: 87.66%
[2025-05-13 23:37:19,404][__main__][INFO] - Val Loss: 0.4595, Val Acc: 84.26%
[2025-05-13 23:37:19,404][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:37:19,404][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-13 23:42:49,221][__main__][INFO] - Epoch 10/30
[2025-05-13 23:42:49,225][__main__][INFO] - Train Loss: 0.3562, Train Acc: 87.77%
[2025-05-13 23:42:49,226][__main__][INFO] - Val Loss: 0.3514, Val Acc: 87.41%
[2025-05-13 23:42:49,226][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:42:49,226][__main__][INFO] - Validation loss improved from 0.3977 to 0.3514
[2025-05-13 23:49:39,969][__main__][INFO] - Epoch 11/30
[2025-05-13 23:49:39,973][__main__][INFO] - Train Loss: 0.3668, Train Acc: 87.82%
[2025-05-13 23:49:39,973][__main__][INFO] - Val Loss: 0.3362, Val Acc: 89.17%
[2025-05-13 23:49:39,973][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:49:39,973][__main__][INFO] - Validation loss improved from 0.3514 to 0.3362
[2025-05-13 23:49:39,998][__main__][INFO] - New best model saved with val acc: 89.17%
[2025-05-13 23:55:44,706][__main__][INFO] - Epoch 12/30
[2025-05-13 23:55:44,710][__main__][INFO] - Train Loss: 0.3580, Train Acc: 88.42%
[2025-05-13 23:55:44,710][__main__][INFO] - Val Loss: 0.4355, Val Acc: 84.89%
[2025-05-13 23:55:44,710][__main__][INFO] - Learning Rate: 0.001000
[2025-05-13 23:55:44,710][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-14 00:02:04,949][__main__][INFO] - Epoch 13/30
[2025-05-14 00:02:04,954][__main__][INFO] - Train Loss: 0.3419, Train Acc: 88.52%
[2025-05-14 00:02:04,954][__main__][INFO] - Val Loss: 0.4481, Val Acc: 84.38%
[2025-05-14 00:02:04,954][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:02:04,955][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-14 00:07:58,958][__main__][INFO] - Epoch 14/30
[2025-05-14 00:07:58,962][__main__][INFO] - Train Loss: 0.3351, Train Acc: 88.55%
[2025-05-14 00:07:58,962][__main__][INFO] - Val Loss: 0.3344, Val Acc: 87.78%
[2025-05-14 00:07:58,962][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:07:58,962][__main__][INFO] - Validation loss improved from 0.3362 to 0.3344
[2025-05-14 00:13:37,345][__main__][INFO] - Epoch 15/30
[2025-05-14 00:13:37,349][__main__][INFO] - Train Loss: 0.3434, Train Acc: 88.23%
[2025-05-14 00:13:37,349][__main__][INFO] - Val Loss: 0.3153, Val Acc: 89.29%
[2025-05-14 00:13:37,349][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:13:37,349][__main__][INFO] - Validation loss improved from 0.3344 to 0.3153
[2025-05-14 00:13:37,372][__main__][INFO] - New best model saved with val acc: 89.29%
[2025-05-14 00:19:38,398][__main__][INFO] - Epoch 16/30
[2025-05-14 00:19:38,402][__main__][INFO] - Train Loss: 0.3168, Train Acc: 89.30%
[2025-05-14 00:19:38,402][__main__][INFO] - Val Loss: 0.3317, Val Acc: 88.66%
[2025-05-14 00:19:38,402][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:19:38,402][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-14 00:25:11,791][__main__][INFO] - Epoch 17/30
[2025-05-14 00:25:11,794][__main__][INFO] - Train Loss: 0.3094, Train Acc: 89.47%
[2025-05-14 00:25:11,794][__main__][INFO] - Val Loss: 0.3123, Val Acc: 90.05%
[2025-05-14 00:25:11,795][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:25:11,795][__main__][INFO] - Validation loss improved from 0.3153 to 0.3123
[2025-05-14 00:25:11,818][__main__][INFO] - New best model saved with val acc: 90.05%
[2025-05-14 00:30:13,374][__main__][INFO] - Epoch 18/30
[2025-05-14 00:30:13,378][__main__][INFO] - Train Loss: 0.3161, Train Acc: 89.22%
[2025-05-14 00:30:13,378][__main__][INFO] - Val Loss: 0.3609, Val Acc: 88.16%
[2025-05-14 00:30:13,378][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:30:13,378][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-14 00:35:36,527][__main__][INFO] - Epoch 19/30
[2025-05-14 00:35:36,531][__main__][INFO] - Train Loss: 0.3077, Train Acc: 89.33%
[2025-05-14 00:35:36,531][__main__][INFO] - Val Loss: 0.3450, Val Acc: 88.54%
[2025-05-14 00:35:36,531][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:35:36,531][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-14 00:41:06,000][__main__][INFO] - Epoch 20/30
[2025-05-14 00:41:06,005][__main__][INFO] - Train Loss: 0.3035, Train Acc: 90.22%
[2025-05-14 00:41:06,005][__main__][INFO] - Val Loss: 0.3905, Val Acc: 86.27%
[2025-05-14 00:41:06,005][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:41:06,005][__main__][INFO] - No improvement in validation loss for 3/10 epochs
[2025-05-14 00:47:31,527][__main__][INFO] - Epoch 21/30
[2025-05-14 00:47:31,531][__main__][INFO] - Train Loss: 0.2911, Train Acc: 90.41%
[2025-05-14 00:47:31,531][__main__][INFO] - Val Loss: 0.3161, Val Acc: 89.55%
[2025-05-14 00:47:31,531][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:47:31,531][__main__][INFO] - No improvement in validation loss for 4/10 epochs
[2025-05-14 00:53:40,125][__main__][INFO] - Epoch 22/30
[2025-05-14 00:53:40,129][__main__][INFO] - Train Loss: 0.2853, Train Acc: 90.57%
[2025-05-14 00:53:40,129][__main__][INFO] - Val Loss: 0.3209, Val Acc: 89.67%
[2025-05-14 00:53:40,129][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 00:53:40,130][__main__][INFO] - No improvement in validation loss for 5/10 epochs
[2025-05-14 01:00:29,956][__main__][INFO] - Epoch 23/30
[2025-05-14 01:00:29,960][__main__][INFO] - Train Loss: 0.2865, Train Acc: 90.09%
[2025-05-14 01:00:29,960][__main__][INFO] - Val Loss: 0.3030, Val Acc: 89.04%
[2025-05-14 01:00:29,961][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 01:00:29,961][__main__][INFO] - Validation loss improved from 0.3123 to 0.3030
[2025-05-14 01:07:51,111][__main__][INFO] - Epoch 24/30
[2025-05-14 01:07:51,115][__main__][INFO] - Train Loss: 0.2711, Train Acc: 90.79%
[2025-05-14 01:07:51,115][__main__][INFO] - Val Loss: 0.3034, Val Acc: 89.17%
[2025-05-14 01:07:51,115][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 01:07:51,115][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-14 01:14:44,586][__main__][INFO] - Epoch 25/30
[2025-05-14 01:14:44,590][__main__][INFO] - Train Loss: 0.2840, Train Acc: 90.44%
[2025-05-14 01:14:44,590][__main__][INFO] - Val Loss: 0.3447, Val Acc: 89.55%
[2025-05-14 01:14:44,590][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 01:14:44,590][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-14 01:21:39,788][__main__][INFO] - Epoch 26/30
[2025-05-14 01:21:39,791][__main__][INFO] - Train Loss: 0.2929, Train Acc: 90.19%
[2025-05-14 01:21:39,791][__main__][INFO] - Val Loss: 0.2789, Val Acc: 89.92%
[2025-05-14 01:21:39,791][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 01:21:39,791][__main__][INFO] - Validation loss improved from 0.3030 to 0.2789
[2025-05-14 01:28:46,757][__main__][INFO] - Epoch 27/30
[2025-05-14 01:28:46,762][__main__][INFO] - Train Loss: 0.2819, Train Acc: 90.65%
[2025-05-14 01:28:46,762][__main__][INFO] - Val Loss: 0.2982, Val Acc: 89.55%
[2025-05-14 01:28:46,762][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 01:28:46,762][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-14 01:36:30,805][__main__][INFO] - Epoch 28/30
[2025-05-14 01:36:30,812][__main__][INFO] - Train Loss: 0.2676, Train Acc: 91.24%
[2025-05-14 01:36:30,812][__main__][INFO] - Val Loss: 0.2982, Val Acc: 89.29%
[2025-05-14 01:36:30,812][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 01:36:30,812][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-14 01:42:39,707][__main__][INFO] - Epoch 29/30
[2025-05-14 01:42:39,711][__main__][INFO] - Train Loss: 0.2576, Train Acc: 91.43%
[2025-05-14 01:42:39,711][__main__][INFO] - Val Loss: 0.3470, Val Acc: 88.54%
[2025-05-14 01:42:39,711][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 01:42:39,711][__main__][INFO] - No improvement in validation loss for 3/10 epochs
[2025-05-14 01:51:55,985][__main__][INFO] - Epoch 30/30
[2025-05-14 01:51:55,988][__main__][INFO] - Train Loss: 0.2634, Train Acc: 91.33%
[2025-05-14 01:51:55,989][__main__][INFO] - Val Loss: 0.3075, Val Acc: 89.29%
[2025-05-14 01:51:55,989][__main__][INFO] - Learning Rate: 0.001000
[2025-05-14 01:51:55,989][__main__][INFO] - No improvement in validation loss for 4/10 epochs
[2025-05-14 01:51:56,952][__main__][INFO] - Training history plot saved to /Users/leonardomannini/Documenti/FBK/repos/bird_classification_edge/logs/4birds_CF_linear_triangular_30epochs/2025-05-13_22-45-46/training_history.png
[2025-05-14 01:51:56,952][__main__][INFO] - 
Evaluating on test set...
[2025-05-14 01:54:02,848][__main__][INFO] - Test Loss: 0.3499, Test Acc: 88.16%
[2025-05-14 01:54:03,255][__main__][INFO] - Confusion matrix saved to /Users/leonardomannini/Documenti/FBK/repos/bird_classification_edge/logs/4birds_CF_linear_triangular_30epochs/2025-05-13_22-45-46/confusion_matrix.png
[2025-05-14 01:54:03,256][__main__][INFO] - Training completed! Best validation accuracy: 90.05%
[2025-05-14 01:54:03,256][__main__][INFO] - Test accuracy: 88.16%
[2025-05-14 01:54:03,256][__main__][INFO] - Results saved to /Users/leonardomannini/Documenti/FBK/repos/bird_classification_edge/logs/4birds_CF_linear_triangular_30epochs/2025-05-13_22-45-46
