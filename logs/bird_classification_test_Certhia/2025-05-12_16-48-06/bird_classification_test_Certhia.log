[2025-05-12 16:48:06,505][__main__][INFO] - Experiment: bird_classification_test_Certhia
[2025-05-12 16:48:06,508][__main__][INFO] - Using device: cpu
[2025-05-12 16:48:06,508][__main__][INFO] - Checking for datasets...
[2025-05-12 16:48:06,508][__main__][INFO] - ESC-50 dataset not found or empty. Downloading...
[2025-05-12 16:48:06,508][__main__][INFO] - Creating datasets...
[2025-05-12 16:48:06,508][__main__][INFO] - Dataset split parameters: val=0.15, test=0.15, seed=42
[2025-05-12 16:48:06,548][__main__][INFO] - Training samples: 3205
[2025-05-12 16:48:06,548][__main__][INFO] - Validation samples: 1314
[2025-05-12 16:48:06,548][__main__][INFO] - Testing samples: 1314
[2025-05-12 16:48:06,557][__main__][INFO] - Model architecture:
[2025-05-12 16:48:06,557][__main__][INFO] - Improved_Phi_GRU_ATT(
  (mel_spec): MelSpectrogram(
    (spectrogram): Spectrogram()
    (mel_scale): MelScale()
  )
  (amplitude_to_db): AmplitudeToDB()
  (phi): MatchboxNetSkip(
    (initial_conv_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(4, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(5,), stride=(1,), groups=64, bias=False)
          )
          (5): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(64, 10, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(10, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (blocks_modulelist): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(8, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(4,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (2): ModuleList(
        (0-1): 2 x Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (projections_modulelist): ModuleList(
      (0-2): 3 x Identity()
    )
    (final_block_to_conv1_projection): Identity()
    (final_conv1_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(8, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(5,), stride=(1,), dilation=(2,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (conv1_to_conv2_projection): Identity()
    (final_conv2_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
  )
  (gru): GRU(32, 32, batch_first=True)
  (projection): Linear(in_features=32, out_features=32, bias=True)
  (keyword_attention): AttentionLayer(
    (attention): Linear(in_features=32, out_features=1, bias=True)
  )
  (fc): Linear(in_features=32, out_features=4, bias=True)
)
[2025-05-12 16:48:06,559][__main__][INFO] - Total parameters: 37,445
[2025-05-12 16:48:06,560][__main__][INFO] - Trainable parameters: 37,445
[2025-05-12 16:48:06,560][__main__][INFO] - Computing model complexity...
[2025-05-12 16:48:40,796][__main__][WARNING] - Could not compute MACs: unsupported format string passed to NoneType.__format__
[2025-05-12 16:48:41,507][__main__][INFO] - Starting training...
[2025-05-12 16:53:40,850][__main__][INFO] - Epoch 1/10
[2025-05-12 16:53:40,855][__main__][INFO] - Train Loss: 0.9592, Train Acc: 61.50%
[2025-05-12 16:53:40,855][__main__][INFO] - Val Loss: 1.0955, Val Acc: 49.32%
[2025-05-12 16:53:40,855][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 16:53:40,855][__main__][INFO] - Validation loss improved from inf to 1.0955
[2025-05-12 16:53:40,889][__main__][INFO] - New best model saved with val acc: 49.32%
[2025-05-12 16:58:18,926][__main__][INFO] - Epoch 2/10
[2025-05-12 16:58:18,928][__main__][INFO] - Train Loss: 0.7398, Train Acc: 71.08%
[2025-05-12 16:58:18,928][__main__][INFO] - Val Loss: 0.7476, Val Acc: 72.68%
[2025-05-12 16:58:18,928][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 16:58:18,928][__main__][INFO] - Validation loss improved from 1.0955 to 0.7476
[2025-05-12 16:58:18,950][__main__][INFO] - New best model saved with val acc: 72.68%
[2025-05-12 17:03:25,733][__main__][INFO] - Epoch 3/10
[2025-05-12 17:03:25,738][__main__][INFO] - Train Loss: 0.7017, Train Acc: 72.11%
[2025-05-12 17:03:25,738][__main__][INFO] - Val Loss: 0.6243, Val Acc: 78.08%
[2025-05-12 17:03:25,738][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 17:03:25,738][__main__][INFO] - Validation loss improved from 0.7476 to 0.6243
[2025-05-12 17:03:25,761][__main__][INFO] - New best model saved with val acc: 78.08%
[2025-05-12 17:08:51,320][__main__][INFO] - Epoch 4/10
[2025-05-12 17:08:51,324][__main__][INFO] - Train Loss: 0.6698, Train Acc: 73.45%
[2025-05-12 17:08:51,325][__main__][INFO] - Val Loss: 0.6790, Val Acc: 72.91%
[2025-05-12 17:08:51,325][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 17:08:51,325][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-12 17:13:59,590][__main__][INFO] - Epoch 5/10
[2025-05-12 17:13:59,593][__main__][INFO] - Train Loss: 0.6406, Train Acc: 76.51%
[2025-05-12 17:13:59,594][__main__][INFO] - Val Loss: 0.7681, Val Acc: 65.68%
[2025-05-12 17:13:59,594][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 17:13:59,594][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-12 17:20:09,733][__main__][INFO] - Epoch 6/10
[2025-05-12 17:20:09,737][__main__][INFO] - Train Loss: 0.6307, Train Acc: 75.91%
[2025-05-12 17:20:09,737][__main__][INFO] - Val Loss: 0.5255, Val Acc: 81.81%
[2025-05-12 17:20:09,737][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 17:20:09,737][__main__][INFO] - Validation loss improved from 0.6243 to 0.5255
[2025-05-12 17:20:09,767][__main__][INFO] - New best model saved with val acc: 81.81%
[2025-05-12 17:26:18,683][__main__][INFO] - Epoch 7/10
[2025-05-12 17:26:18,687][__main__][INFO] - Train Loss: 0.6190, Train Acc: 76.85%
[2025-05-12 17:26:18,687][__main__][INFO] - Val Loss: 0.5646, Val Acc: 80.44%
[2025-05-12 17:26:18,687][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 17:26:18,688][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-12 17:32:26,231][__main__][INFO] - Epoch 8/10
[2025-05-12 17:32:26,234][__main__][INFO] - Train Loss: 0.5793, Train Acc: 78.63%
[2025-05-12 17:32:26,234][__main__][INFO] - Val Loss: 0.5485, Val Acc: 78.16%
[2025-05-12 17:32:26,234][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 17:32:26,234][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-12 17:39:40,134][__main__][INFO] - Epoch 9/10
[2025-05-12 17:39:40,137][__main__][INFO] - Train Loss: 0.5786, Train Acc: 78.56%
[2025-05-12 17:39:40,137][__main__][INFO] - Val Loss: 0.4812, Val Acc: 82.88%
[2025-05-12 17:39:40,137][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 17:39:40,137][__main__][INFO] - Validation loss improved from 0.5255 to 0.4812
[2025-05-12 17:39:40,165][__main__][INFO] - New best model saved with val acc: 82.88%
[2025-05-12 17:46:12,333][__main__][INFO] - Epoch 10/10
[2025-05-12 17:46:12,338][__main__][INFO] - Train Loss: 0.5474, Train Acc: 80.28%
[2025-05-12 17:46:12,338][__main__][INFO] - Val Loss: 0.6453, Val Acc: 77.09%
[2025-05-12 17:46:12,338][__main__][INFO] - Learning Rate: 0.001000
[2025-05-12 17:46:12,338][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-12 17:46:12,975][__main__][INFO] - Training history plot saved to /Users/leonardomannini/Documenti/FBK/repos/bird_classification_edge/logs/bird_classification_test_Certhia/2025-05-12_16-48-06/training_history.png
[2025-05-12 17:46:12,976][__main__][INFO] - 
Evaluating on test set...
[2025-05-12 17:47:30,396][__main__][INFO] - Test Loss: 0.4812, Test Acc: 82.65%
[2025-05-12 17:47:30,745][__main__][INFO] - Confusion matrix saved to /Users/leonardomannini/Documenti/FBK/repos/bird_classification_edge/logs/bird_classification_test_Certhia/2025-05-12_16-48-06/confusion_matrix.png
[2025-05-12 17:47:30,746][__main__][INFO] - Training completed! Best validation accuracy: 82.88%
[2025-05-12 17:47:30,747][__main__][INFO] - Test accuracy: 82.65%
[2025-05-12 17:47:30,747][__main__][INFO] - Results saved to /Users/leonardomannini/Documenti/FBK/repos/bird_classification_edge/logs/bird_classification_test_Certhia/2025-05-12_16-48-06
