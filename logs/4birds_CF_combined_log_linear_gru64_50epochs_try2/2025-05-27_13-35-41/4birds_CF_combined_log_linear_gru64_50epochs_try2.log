[2025-05-27 13:35:41,418][__main__][INFO] - Experiment: 4birds_CF_combined_log_linear_gru64_50epochs_try2
[2025-05-27 13:35:41,472][__main__][INFO] - Using device: cuda
[2025-05-27 13:35:41,472][__main__][INFO] - Checking for datasets...
[2025-05-27 13:35:41,472][__main__][INFO] - ESC-50 dataset found at: ESC-50-master
[2025-05-27 13:35:41,472][__main__][INFO] - Creating initial bird sound datasets (train/val/test)...
[2025-05-27 13:35:41,473][__main__][INFO] - Using dataset split parameters: val=0.15, test=0.15, seed=42
[2025-05-27 13:35:41,595][__main__][INFO] - Initial bird samples: Train=6996, Val=1495, Test=1495
[2025-05-27 13:35:41,595][__main__][INFO] - Calculating target number of 'no birds' samples...
[2025-05-27 13:35:41,595][__main__][INFO] - Number of bird classes: 8. 'No Birds' label index: 8
[2025-05-27 13:35:41,598][__main__][INFO] - Average samples per bird class in training set: 874.50
[2025-05-27 13:35:41,598][__main__][INFO] - Target 'no birds' samples for training: 874
[2025-05-27 13:35:41,598][__main__][INFO] - Target 'no birds' samples for validation: 187
[2025-05-27 13:35:41,598][__main__][INFO] - Target 'no birds' samples for testing: 187
[2025-05-27 13:35:41,625][__main__][INFO] - Combining bird and 'no birds' datasets...
[2025-05-27 13:35:41,625][__main__][INFO] - Final training samples: 7832
[2025-05-27 13:35:41,625][__main__][INFO] - Final validation samples: 1682
[2025-05-27 13:35:41,625][__main__][INFO] - Final testing samples: 1682
[2025-05-27 13:35:41,843][__main__][INFO] - Using specific LR for breakpoint parameters: 3.0.
[2025-05-27 13:35:41,843][__main__][INFO] - Using specific LR for transition_width parameters: 0.1.
[2025-05-27 13:35:43,455][__main__][INFO] - Model architecture:
[2025-05-27 13:35:43,458][__main__][INFO] - Improved_Phi_GRU_ATT(
  (combined_log_linear_spec): DifferentiableSpectrogram()
  (amplitude_to_db): AmplitudeToDB()
  (phi): MatchboxNetSkip(
    (initial_conv_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(4, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(5,), stride=(1,), groups=64, bias=False)
          )
          (5): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(64, 10, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(10, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (blocks_modulelist): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(8, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(4,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (2): ModuleList(
        (0-1): 2 x Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (projections_modulelist): ModuleList(
      (0-2): 3 x Identity()
    )
    (final_block_to_conv1_projection): Identity()
    (final_conv1_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(8, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(5,), stride=(1,), dilation=(2,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (conv1_to_conv2_projection): Identity()
    (final_conv2_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
  )
  (gru): GRU(32, 64, batch_first=True)
  (projection): Linear(in_features=64, out_features=64, bias=True)
  (keyword_attention): AttentionLayer(
    (attention): Linear(in_features=64, out_features=1, bias=True)
  )
  (fc): Linear(in_features=64, out_features=9, bias=True)
)
[2025-05-27 13:35:43,460][__main__][INFO] - Total parameters: 53,516
[2025-05-27 13:35:43,460][__main__][INFO] - Trainable parameters: 53,516
[2025-05-27 13:35:43,461][__main__][INFO] - Computing model complexity...
[2025-05-27 13:35:43,461][__main__][INFO] - Model MACs: None
[2025-05-27 13:35:43,461][__main__][INFO] - Starting training...
[2025-05-27 13:45:09,450][__main__][INFO] - Epoch 1 - Breakpoint: 3768.69 Hz, Transition Width: 95.59
[2025-05-27 13:45:09,450][__main__][INFO] - Epoch 1/2
[2025-05-27 13:45:09,450][__main__][INFO] - Train Loss: 1.4928, Train Acc: 47.59%
[2025-05-27 13:45:09,451][__main__][INFO] - Val Loss: 1.0763, Val Acc: 64.98%
[2025-05-27 13:45:09,451][__main__][INFO] - Val Precision (w): 0.6514, Val Recall (w): 0.6498, Val F1 (w): 0.6315
[2025-05-27 13:45:09,451][__main__][INFO] - Learning Rate: 0.001000
[2025-05-27 13:45:09,481][__main__][INFO] - Validation loss improved from 1.0773 to 1.0763
[2025-05-27 13:45:09,482][__main__][INFO] - New best model saved with val acc: 64.98%
[2025-05-27 13:55:01,147][__main__][INFO] - Epoch 2 - Breakpoint: 3620.61 Hz, Transition Width: 88.88
[2025-05-27 13:55:01,147][__main__][INFO] - Epoch 2/2
[2025-05-27 13:55:01,147][__main__][INFO] - Train Loss: 1.0906, Train Acc: 64.13%
[2025-05-27 13:55:01,148][__main__][INFO] - Val Loss: 0.8781, Val Acc: 72.12%
[2025-05-27 13:55:01,148][__main__][INFO] - Val Precision (w): 0.7175, Val Recall (w): 0.7212, Val F1 (w): 0.7101
[2025-05-27 13:55:01,148][__main__][INFO] - Learning Rate: 0.001000
[2025-05-27 13:55:01,190][__main__][INFO] - Validation loss improved from 0.8791 to 0.8781
[2025-05-27 13:55:01,190][__main__][INFO] - New best model saved with val acc: 72.12%
[2025-05-27 13:55:02,796][__main__][INFO] - Training history plot saved to /app/logs/4birds_CF_combined_log_linear_gru64_50epochs_try2/2025-05-27_13-35-41/training_history.png
[2025-05-27 13:55:02,796][__main__][INFO] - 
Evaluating on test set...
[2025-05-27 13:55:02,881][__main__][INFO] - Loaded best model based on validation loss for testing.
[2025-05-27 13:56:33,550][__main__][INFO] - Test Loss: 0.8826, Test Acc: 72.41%
[2025-05-27 13:56:33,555][__main__][INFO] - Test Precision (w): 0.7228, Test Recall (w): 0.7241, Test F1 (w): 0.7124
[2025-05-27 13:56:33,556][__main__][WARNING] - 'class_labels' not defined for classification_report. Using numeric labels.
[2025-05-27 13:56:33,565][__main__][INFO] - Classification Report (Test Set):
              precision    recall  f1-score   support

           0       0.63      0.78      0.70       129
           1       0.67      0.83      0.74       198
           2       0.84      0.84      0.84       121
           3       0.79      0.68      0.73       209
           4       0.58      0.23      0.33        60
           5       0.73      0.39      0.51       156
           6       0.72      0.81      0.76       435
           7       0.62      0.55      0.58       187
           8       0.84      0.96      0.90       187

    accuracy                           0.72      1682
   macro avg       0.71      0.67      0.68      1682
weighted avg       0.72      0.72      0.71      1682

[2025-05-27 13:56:33,571][__main__][INFO] - Confusion matrix CSV saved to /app/logs/4birds_CF_combined_log_linear_gru64_50epochs_try2/2025-05-27_13-35-41/confusion_matrix.csv
[2025-05-27 13:56:33,851][__main__][INFO] - Confusion matrix PNG saved to /app/logs/4birds_CF_combined_log_linear_gru64_50epochs_try2/2025-05-27_13-35-41/confusion_matrix.png
[2025-05-27 13:56:33,852][__main__][INFO] - Results saved to /app/logs/4birds_CF_combined_log_linear_gru64_50epochs_try2/2025-05-27_13-35-41/results.json
[2025-05-27 13:56:33,852][__main__][INFO] - Model summary saved to /app/logs/4birds_CF_combined_log_linear_gru64_50epochs_try2/2025-05-27_13-35-41/model_summary.txt
[2025-05-27 13:56:33,852][__main__][INFO] - Training completed! Best validation accuracy: 72.12%
[2025-05-27 13:56:33,852][__main__][INFO] - Test accuracy: 72.41%
[2025-05-27 13:56:33,852][__main__][INFO] - Output directory: /app/logs/4birds_CF_combined_log_linear_gru64_50epochs_try2/2025-05-27_13-35-41
