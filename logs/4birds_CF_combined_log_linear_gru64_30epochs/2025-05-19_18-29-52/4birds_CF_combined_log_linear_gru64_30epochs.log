[2025-05-19 18:29:52,785][__main__][INFO] - Experiment: 4birds_CF_combined_log_linear_gru64_30epochs
[2025-05-19 18:29:52,804][__main__][INFO] - Using device: cuda
[2025-05-19 18:29:52,804][__main__][INFO] - Checking for datasets...
[2025-05-19 18:29:52,804][__main__][INFO] - ESC-50 dataset not found or empty. Downloading...
[2025-05-19 18:29:52,804][__main__][INFO] - Creating initial bird sound datasets (train/val/test)...
[2025-05-19 18:29:52,804][__main__][INFO] - Using dataset split parameters: val=0.15, test=0.15, seed=42
[2025-05-19 18:29:52,828][__main__][INFO] - Initial bird samples: Train=2970, Val=635, Test=635
[2025-05-19 18:29:52,828][__main__][INFO] - Calculating target number of 'no birds' samples...
[2025-05-19 18:29:52,828][__main__][INFO] - Number of bird classes: 4. 'No Birds' label index: 4
[2025-05-19 18:29:52,829][__main__][INFO] - Average samples per bird class in training set: 742.50
[2025-05-19 18:29:52,829][__main__][INFO] - Target 'no birds' samples for training: 742
[2025-05-19 18:29:52,829][__main__][INFO] - Target 'no birds' samples for validation: 159
[2025-05-19 18:29:52,829][__main__][INFO] - Target 'no birds' samples for testing: 159
[2025-05-19 18:29:52,830][__main__][INFO] - Combining bird and 'no birds' datasets...
[2025-05-19 18:29:52,830][__main__][INFO] - Final training samples: 2970
[2025-05-19 18:29:52,830][__main__][INFO] - Final validation samples: 635
[2025-05-19 18:29:52,830][__main__][INFO] - Final testing samples: 635
[2025-05-19 18:29:52,993][__main__][INFO] - Model architecture:
[2025-05-19 18:29:52,994][__main__][INFO] - Improved_Phi_GRU_ATT(
  (combined_log_linear_spec): DifferentiableSpectrogram()
  (amplitude_to_db): AmplitudeToDB()
  (phi): MatchboxNetSkip(
    (initial_conv_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(4, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(5,), stride=(1,), groups=64, bias=False)
          )
          (5): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(64, 10, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(10, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (blocks_modulelist): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(8, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(4,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (2): ModuleList(
        (0-1): 2 x Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (projections_modulelist): ModuleList(
      (0-2): 3 x Identity()
    )
    (final_block_to_conv1_projection): Identity()
    (final_conv1_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(8, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(5,), stride=(1,), dilation=(2,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (conv1_to_conv2_projection): Identity()
    (final_conv2_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
  )
  (gru): GRU(32, 64, batch_first=True)
  (projection): Linear(in_features=64, out_features=64, bias=True)
  (keyword_attention): AttentionLayer(
    (attention): Linear(in_features=64, out_features=1, bias=True)
  )
  (fc): Linear(in_features=64, out_features=5, bias=True)
)
[2025-05-19 18:29:52,998][__main__][INFO] - Total parameters: 53,256
[2025-05-19 18:29:52,998][__main__][INFO] - Trainable parameters: 53,256
[2025-05-19 18:29:52,998][__main__][INFO] - Computing model complexity...
[2025-05-19 18:30:08,674][__main__][WARNING] - Could not compute MACs: unsupported format string passed to NoneType.__format__
[2025-05-19 18:30:09,505][__main__][INFO] - Starting training...
[2025-05-19 18:34:35,298][__main__][INFO] - Epoch 1/30
[2025-05-19 18:34:35,298][__main__][INFO] - Train Loss: 1.3754, Train Acc: 33.20%
[2025-05-19 18:34:35,298][__main__][INFO] - Val Loss: 1.2882, Val Acc: 33.23%
[2025-05-19 18:34:35,298][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 18:34:35,298][__main__][INFO] - Validation loss improved from inf to 1.2882
[2025-05-19 18:34:35,315][__main__][INFO] - New best model saved with val acc: 33.23%
[2025-05-19 18:38:59,114][__main__][INFO] - Epoch 2/30
[2025-05-19 18:38:59,114][__main__][INFO] - Train Loss: 1.2942, Train Acc: 35.76%
[2025-05-19 18:38:59,114][__main__][INFO] - Val Loss: 1.2158, Val Acc: 45.35%
[2025-05-19 18:38:59,114][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 18:38:59,114][__main__][INFO] - Validation loss improved from 1.2882 to 1.2158
[2025-05-19 18:38:59,138][__main__][INFO] - New best model saved with val acc: 45.35%
[2025-05-19 18:43:21,377][__main__][INFO] - Epoch 3/30
[2025-05-19 18:43:21,377][__main__][INFO] - Train Loss: 1.2306, Train Acc: 42.56%
[2025-05-19 18:43:21,377][__main__][INFO] - Val Loss: 1.2237, Val Acc: 47.09%
[2025-05-19 18:43:21,377][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 18:43:21,377][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-19 18:43:21,395][__main__][INFO] - New best model saved with val acc: 47.09%
[2025-05-19 18:47:36,668][__main__][INFO] - Epoch 4/30
[2025-05-19 18:47:36,668][__main__][INFO] - Train Loss: 1.1899, Train Acc: 45.93%
[2025-05-19 18:47:36,668][__main__][INFO] - Val Loss: 1.1197, Val Acc: 49.29%
[2025-05-19 18:47:36,668][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 18:47:36,669][__main__][INFO] - Validation loss improved from 1.2158 to 1.1197
[2025-05-19 18:47:36,685][__main__][INFO] - New best model saved with val acc: 49.29%
[2025-05-19 18:52:00,988][__main__][INFO] - Epoch 5/30
[2025-05-19 18:52:00,988][__main__][INFO] - Train Loss: 1.1292, Train Acc: 51.35%
[2025-05-19 18:52:00,988][__main__][INFO] - Val Loss: 1.0626, Val Acc: 52.91%
[2025-05-19 18:52:00,989][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 18:52:00,989][__main__][INFO] - Validation loss improved from 1.1197 to 1.0626
[2025-05-19 18:52:01,024][__main__][INFO] - New best model saved with val acc: 52.91%
[2025-05-19 18:56:14,690][__main__][INFO] - Epoch 6/30
[2025-05-19 18:56:14,690][__main__][INFO] - Train Loss: 1.0872, Train Acc: 53.67%
[2025-05-19 18:56:14,690][__main__][INFO] - Val Loss: 1.0596, Val Acc: 54.17%
[2025-05-19 18:56:14,690][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 18:56:14,690][__main__][INFO] - Validation loss improved from 1.0626 to 1.0596
[2025-05-19 18:56:14,710][__main__][INFO] - New best model saved with val acc: 54.17%
[2025-05-19 19:00:38,833][__main__][INFO] - Epoch 7/30
[2025-05-19 19:00:38,833][__main__][INFO] - Train Loss: 1.0154, Train Acc: 57.41%
[2025-05-19 19:00:38,833][__main__][INFO] - Val Loss: 0.9912, Val Acc: 61.10%
[2025-05-19 19:00:38,833][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:00:38,833][__main__][INFO] - Validation loss improved from 1.0596 to 0.9912
[2025-05-19 19:00:38,855][__main__][INFO] - New best model saved with val acc: 61.10%
[2025-05-19 19:05:07,597][__main__][INFO] - Epoch 8/30
[2025-05-19 19:05:07,598][__main__][INFO] - Train Loss: 0.9816, Train Acc: 58.62%
[2025-05-19 19:05:07,598][__main__][INFO] - Val Loss: 0.9703, Val Acc: 59.69%
[2025-05-19 19:05:07,598][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:05:07,598][__main__][INFO] - Validation loss improved from 0.9912 to 0.9703
[2025-05-19 19:09:26,146][__main__][INFO] - Epoch 9/30
[2025-05-19 19:09:26,146][__main__][INFO] - Train Loss: 0.9291, Train Acc: 61.28%
[2025-05-19 19:09:26,147][__main__][INFO] - Val Loss: 0.9681, Val Acc: 60.94%
[2025-05-19 19:09:26,147][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:09:26,147][__main__][INFO] - Validation loss improved from 0.9703 to 0.9681
[2025-05-19 19:14:01,740][__main__][INFO] - Epoch 10/30
[2025-05-19 19:14:01,740][__main__][INFO] - Train Loss: 0.9122, Train Acc: 62.29%
[2025-05-19 19:14:01,741][__main__][INFO] - Val Loss: 0.8697, Val Acc: 65.04%
[2025-05-19 19:14:01,741][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:14:01,741][__main__][INFO] - Validation loss improved from 0.9681 to 0.8697
[2025-05-19 19:14:01,758][__main__][INFO] - New best model saved with val acc: 65.04%
[2025-05-19 19:18:24,871][__main__][INFO] - Epoch 11/30
[2025-05-19 19:18:24,871][__main__][INFO] - Train Loss: 0.8832, Train Acc: 63.27%
[2025-05-19 19:18:24,871][__main__][INFO] - Val Loss: 0.8340, Val Acc: 65.98%
[2025-05-19 19:18:24,871][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:18:24,871][__main__][INFO] - Validation loss improved from 0.8697 to 0.8340
[2025-05-19 19:18:24,890][__main__][INFO] - New best model saved with val acc: 65.98%
[2025-05-19 19:22:42,983][__main__][INFO] - Epoch 12/30
[2025-05-19 19:22:42,984][__main__][INFO] - Train Loss: 0.8572, Train Acc: 64.71%
[2025-05-19 19:22:42,984][__main__][INFO] - Val Loss: 0.8527, Val Acc: 65.83%
[2025-05-19 19:22:42,984][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:22:42,984][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-19 19:27:11,531][__main__][INFO] - Epoch 13/30
[2025-05-19 19:27:11,531][__main__][INFO] - Train Loss: 0.8547, Train Acc: 65.29%
[2025-05-19 19:27:11,531][__main__][INFO] - Val Loss: 0.8121, Val Acc: 68.66%
[2025-05-19 19:27:11,531][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:27:11,531][__main__][INFO] - Validation loss improved from 0.8340 to 0.8121
[2025-05-19 19:27:11,554][__main__][INFO] - New best model saved with val acc: 68.66%
[2025-05-19 19:31:28,690][__main__][INFO] - Epoch 14/30
[2025-05-19 19:31:28,690][__main__][INFO] - Train Loss: 0.8336, Train Acc: 66.23%
[2025-05-19 19:31:28,690][__main__][INFO] - Val Loss: 0.8215, Val Acc: 65.98%
[2025-05-19 19:31:28,690][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:31:28,690][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-19 19:35:48,555][__main__][INFO] - Epoch 15/30
[2025-05-19 19:35:48,555][__main__][INFO] - Train Loss: 0.8099, Train Acc: 67.24%
[2025-05-19 19:35:48,555][__main__][INFO] - Val Loss: 0.7767, Val Acc: 68.50%
[2025-05-19 19:35:48,556][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:35:48,556][__main__][INFO] - Validation loss improved from 0.8121 to 0.7767
[2025-05-19 19:40:14,853][__main__][INFO] - Epoch 16/30
[2025-05-19 19:40:14,853][__main__][INFO] - Train Loss: 0.7934, Train Acc: 68.28%
[2025-05-19 19:40:14,853][__main__][INFO] - Val Loss: 0.7901, Val Acc: 68.82%
[2025-05-19 19:40:14,853][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:40:14,853][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-19 19:40:14,871][__main__][INFO] - New best model saved with val acc: 68.82%
[2025-05-19 19:44:46,765][__main__][INFO] - Epoch 17/30
[2025-05-19 19:44:46,765][__main__][INFO] - Train Loss: 0.7913, Train Acc: 68.01%
[2025-05-19 19:44:46,765][__main__][INFO] - Val Loss: 0.8054, Val Acc: 67.40%
[2025-05-19 19:44:46,765][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:44:46,765][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-19 19:49:35,771][__main__][INFO] - Epoch 18/30
[2025-05-19 19:49:35,771][__main__][INFO] - Train Loss: 0.7927, Train Acc: 67.61%
[2025-05-19 19:49:35,771][__main__][INFO] - Val Loss: 0.8043, Val Acc: 66.30%
[2025-05-19 19:49:35,771][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:49:35,771][__main__][INFO] - No improvement in validation loss for 3/10 epochs
[2025-05-19 19:53:54,146][__main__][INFO] - Epoch 19/30
[2025-05-19 19:53:54,146][__main__][INFO] - Train Loss: 0.7669, Train Acc: 69.23%
[2025-05-19 19:53:54,146][__main__][INFO] - Val Loss: 0.7387, Val Acc: 72.44%
[2025-05-19 19:53:54,146][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:53:54,146][__main__][INFO] - Validation loss improved from 0.7767 to 0.7387
[2025-05-19 19:53:54,166][__main__][INFO] - New best model saved with val acc: 72.44%
[2025-05-19 19:58:24,146][__main__][INFO] - Epoch 20/30
[2025-05-19 19:58:24,147][__main__][INFO] - Train Loss: 0.7668, Train Acc: 68.65%
[2025-05-19 19:58:24,147][__main__][INFO] - Val Loss: 0.7130, Val Acc: 73.07%
[2025-05-19 19:58:24,147][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 19:58:24,147][__main__][INFO] - Validation loss improved from 0.7387 to 0.7130
[2025-05-19 19:58:24,175][__main__][INFO] - New best model saved with val acc: 73.07%
[2025-05-19 20:02:47,207][__main__][INFO] - Epoch 21/30
[2025-05-19 20:02:47,207][__main__][INFO] - Train Loss: 0.7394, Train Acc: 70.40%
[2025-05-19 20:02:47,207][__main__][INFO] - Val Loss: 0.7321, Val Acc: 71.02%
[2025-05-19 20:02:47,207][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:02:47,207][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-19 20:07:07,901][__main__][INFO] - Epoch 22/30
[2025-05-19 20:07:07,902][__main__][INFO] - Train Loss: 0.7278, Train Acc: 71.25%
[2025-05-19 20:07:07,902][__main__][INFO] - Val Loss: 0.7639, Val Acc: 71.02%
[2025-05-19 20:07:07,902][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:07:07,902][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-19 20:11:39,426][__main__][INFO] - Epoch 23/30
[2025-05-19 20:11:39,427][__main__][INFO] - Train Loss: 0.7239, Train Acc: 70.51%
[2025-05-19 20:11:39,427][__main__][INFO] - Val Loss: 0.7240, Val Acc: 72.44%
[2025-05-19 20:11:39,427][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:11:39,427][__main__][INFO] - No improvement in validation loss for 3/10 epochs
[2025-05-19 20:15:57,069][__main__][INFO] - Epoch 24/30
[2025-05-19 20:15:57,069][__main__][INFO] - Train Loss: 0.6999, Train Acc: 71.95%
[2025-05-19 20:15:57,069][__main__][INFO] - Val Loss: 0.7161, Val Acc: 72.44%
[2025-05-19 20:15:57,070][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:15:57,070][__main__][INFO] - No improvement in validation loss for 4/10 epochs
[2025-05-19 20:20:15,767][__main__][INFO] - Epoch 25/30
[2025-05-19 20:20:15,767][__main__][INFO] - Train Loss: 0.7233, Train Acc: 71.04%
[2025-05-19 20:20:15,767][__main__][INFO] - Val Loss: 0.6802, Val Acc: 73.54%
[2025-05-19 20:20:15,768][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:20:15,768][__main__][INFO] - Validation loss improved from 0.7130 to 0.6802
[2025-05-19 20:20:15,787][__main__][INFO] - New best model saved with val acc: 73.54%
[2025-05-19 20:24:56,200][__main__][INFO] - Epoch 26/30
[2025-05-19 20:24:56,201][__main__][INFO] - Train Loss: 0.6980, Train Acc: 72.09%
[2025-05-19 20:24:56,201][__main__][INFO] - Val Loss: 0.7012, Val Acc: 72.13%
[2025-05-19 20:24:56,201][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:24:56,201][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-19 20:29:31,482][__main__][INFO] - Epoch 27/30
[2025-05-19 20:29:31,482][__main__][INFO] - Train Loss: 0.6935, Train Acc: 72.59%
[2025-05-19 20:29:31,482][__main__][INFO] - Val Loss: 0.6867, Val Acc: 75.12%
[2025-05-19 20:29:31,482][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:29:31,482][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-19 20:29:31,500][__main__][INFO] - New best model saved with val acc: 75.12%
[2025-05-19 20:33:49,657][__main__][INFO] - Epoch 28/30
[2025-05-19 20:33:49,658][__main__][INFO] - Train Loss: 0.6734, Train Acc: 74.58%
[2025-05-19 20:33:49,658][__main__][INFO] - Val Loss: 0.6770, Val Acc: 72.91%
[2025-05-19 20:33:49,658][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:33:49,658][__main__][INFO] - Validation loss improved from 0.6802 to 0.6770
[2025-05-19 20:38:14,645][__main__][INFO] - Epoch 29/30
[2025-05-19 20:38:14,645][__main__][INFO] - Train Loss: 0.6923, Train Acc: 72.49%
[2025-05-19 20:38:14,645][__main__][INFO] - Val Loss: 0.7383, Val Acc: 71.81%
[2025-05-19 20:38:14,645][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:38:14,645][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-19 20:42:45,884][__main__][INFO] - Epoch 30/30
[2025-05-19 20:42:45,885][__main__][INFO] - Train Loss: 0.6617, Train Acc: 74.21%
[2025-05-19 20:42:45,885][__main__][INFO] - Val Loss: 0.7386, Val Acc: 73.70%
[2025-05-19 20:42:45,885][__main__][INFO] - Learning Rate: 0.001000
[2025-05-19 20:42:45,885][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-19 20:42:46,630][__main__][INFO] - Training history plot saved to /raid/home/e3da/interns/lmannini/bird_classification_edge/logs/4birds_CF_combined_log_linear_gru64_30epochs/2025-05-19_18-29-52/training_history.png
[2025-05-19 20:42:46,630][__main__][INFO] - 
Evaluating on test set...
[2025-05-19 20:43:36,311][__main__][INFO] - Test Loss: 0.7059, Test Acc: 71.81%
[2025-05-19 20:43:37,048][__main__][INFO] - Confusion matrix saved to /raid/home/e3da/interns/lmannini/bird_classification_edge/logs/4birds_CF_combined_log_linear_gru64_30epochs/2025-05-19_18-29-52/confusion_matrix.png
[2025-05-19 20:43:37,049][__main__][INFO] - Training completed! Best validation accuracy: 75.12%
[2025-05-19 20:43:37,049][__main__][INFO] - Test accuracy: 71.81%
[2025-05-19 20:43:37,049][__main__][INFO] - Results saved to /raid/home/e3da/interns/lmannini/bird_classification_edge/logs/4birds_CF_combined_log_linear_gru64_30epochs/2025-05-19_18-29-52
