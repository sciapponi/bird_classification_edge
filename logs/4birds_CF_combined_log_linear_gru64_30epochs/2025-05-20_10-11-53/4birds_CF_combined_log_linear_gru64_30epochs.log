[2025-05-20 10:11:53,438][__main__][INFO] - Experiment: 4birds_CF_combined_log_linear_gru64_30epochs
[2025-05-20 10:11:53,456][__main__][INFO] - Using device: cuda
[2025-05-20 10:11:53,457][__main__][INFO] - Checking for datasets...
[2025-05-20 10:11:53,457][__main__][INFO] - ESC-50 dataset not found or empty. Downloading...
[2025-05-20 10:11:53,457][__main__][INFO] - Creating initial bird sound datasets (train/val/test)...
[2025-05-20 10:11:53,457][__main__][INFO] - Using dataset split parameters: val=0.15, test=0.15, seed=42
[2025-05-20 10:11:53,488][__main__][INFO] - Initial bird samples: Train=2970, Val=635, Test=635
[2025-05-20 10:11:53,488][__main__][INFO] - Calculating target number of 'no birds' samples...
[2025-05-20 10:11:53,488][__main__][INFO] - Number of bird classes: 4. 'No Birds' label index: 4
[2025-05-20 10:11:53,489][__main__][INFO] - Average samples per bird class in training set: 742.50
[2025-05-20 10:11:53,489][__main__][INFO] - Target 'no birds' samples for training: 742
[2025-05-20 10:11:53,489][__main__][INFO] - Target 'no birds' samples for validation: 159
[2025-05-20 10:11:53,489][__main__][INFO] - Target 'no birds' samples for testing: 159
[2025-05-20 10:11:53,503][__main__][INFO] - Combining bird and 'no birds' datasets...
[2025-05-20 10:11:53,504][__main__][INFO] - Final training samples: 3712
[2025-05-20 10:11:53,504][__main__][INFO] - Final validation samples: 794
[2025-05-20 10:11:53,504][__main__][INFO] - Final testing samples: 794
[2025-05-20 10:11:54,380][__main__][INFO] - Model architecture:
[2025-05-20 10:11:54,381][__main__][INFO] - Improved_Phi_GRU_ATT(
  (combined_log_linear_spec): DifferentiableSpectrogram()
  (amplitude_to_db): AmplitudeToDB()
  (phi): MatchboxNetSkip(
    (initial_conv_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(4, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(5,), stride=(1,), groups=64, bias=False)
          )
          (5): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(64, 10, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(10, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (blocks_modulelist): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(8, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(4,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (2): ModuleList(
        (0-1): 2 x Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (projections_modulelist): ModuleList(
      (0-2): 3 x Identity()
    )
    (final_block_to_conv1_projection): Identity()
    (final_conv1_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(8, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(5,), stride=(1,), dilation=(2,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (conv1_to_conv2_projection): Identity()
    (final_conv2_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
  )
  (gru): GRU(32, 64, batch_first=True)
  (projection): Linear(in_features=64, out_features=64, bias=True)
  (keyword_attention): AttentionLayer(
    (attention): Linear(in_features=64, out_features=1, bias=True)
  )
  (fc): Linear(in_features=64, out_features=5, bias=True)
)
[2025-05-20 10:11:54,383][__main__][INFO] - Total parameters: 53,256
[2025-05-20 10:11:54,383][__main__][INFO] - Trainable parameters: 53,256
[2025-05-20 10:11:54,383][__main__][INFO] - Computing model complexity...
[2025-05-20 10:11:54,383][__main__][INFO] - Model MACs: None
[2025-05-20 10:11:54,383][__main__][INFO] - Starting training...
[2025-05-20 10:16:11,651][__main__][INFO] - Epoch 1 - Breakpoint: 3999.94 Hz, Transition Width: 99.99
[2025-05-20 10:16:11,652][__main__][INFO] - Epoch 1/30
[2025-05-20 10:16:11,652][__main__][INFO] - Train Loss: 0.8855, Train Acc: 66.57%
[2025-05-20 10:16:11,652][__main__][INFO] - Val Loss: 0.5383, Val Acc: 81.11%
[2025-05-20 10:16:11,652][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:16:11,691][__main__][INFO] - Validation loss improved from 0.5393 to 0.5383
[2025-05-20 10:16:11,691][__main__][INFO] - New best model saved with val acc: 81.11%
[2025-05-20 10:20:25,903][__main__][INFO] - Epoch 2 - Breakpoint: 3999.88 Hz, Transition Width: 99.97
[2025-05-20 10:20:25,903][__main__][INFO] - Epoch 2/30
[2025-05-20 10:20:25,903][__main__][INFO] - Train Loss: 0.5474, Train Acc: 80.85%
[2025-05-20 10:20:25,903][__main__][INFO] - Val Loss: 0.4863, Val Acc: 84.63%
[2025-05-20 10:20:25,903][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:20:25,941][__main__][INFO] - Validation loss improved from 0.4873 to 0.4863
[2025-05-20 10:20:25,941][__main__][INFO] - New best model saved with val acc: 84.63%
[2025-05-20 10:24:46,039][__main__][INFO] - Epoch 3 - Breakpoint: 3999.80 Hz, Transition Width: 99.94
[2025-05-20 10:24:46,040][__main__][INFO] - Epoch 3/30
[2025-05-20 10:24:46,040][__main__][INFO] - Train Loss: 0.4891, Train Acc: 83.73%
[2025-05-20 10:24:46,040][__main__][INFO] - Val Loss: 0.4253, Val Acc: 86.15%
[2025-05-20 10:24:46,040][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:24:46,056][__main__][INFO] - Validation loss improved from 0.4263 to 0.4253
[2025-05-20 10:24:46,056][__main__][INFO] - New best model saved with val acc: 86.15%
[2025-05-20 10:29:00,569][__main__][INFO] - Epoch 4 - Breakpoint: 3999.75 Hz, Transition Width: 99.93
[2025-05-20 10:29:00,569][__main__][INFO] - Epoch 4/30
[2025-05-20 10:29:00,569][__main__][INFO] - Train Loss: 0.4458, Train Acc: 84.91%
[2025-05-20 10:29:00,569][__main__][INFO] - Val Loss: 0.3551, Val Acc: 88.04%
[2025-05-20 10:29:00,570][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:29:00,602][__main__][INFO] - Validation loss improved from 0.3561 to 0.3551
[2025-05-20 10:29:00,603][__main__][INFO] - New best model saved with val acc: 88.04%
[2025-05-20 10:33:29,148][__main__][INFO] - Epoch 5 - Breakpoint: 3999.68 Hz, Transition Width: 99.92
[2025-05-20 10:33:29,149][__main__][INFO] - Epoch 5/30
[2025-05-20 10:33:29,149][__main__][INFO] - Train Loss: 0.4574, Train Acc: 84.32%
[2025-05-20 10:33:29,149][__main__][INFO] - Val Loss: 0.3927, Val Acc: 86.52%
[2025-05-20 10:33:29,149][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:33:29,149][__main__][INFO] - No improvement in validation loss for 1/4 epochs
[2025-05-20 10:37:56,028][__main__][INFO] - Epoch 6 - Breakpoint: 3999.61 Hz, Transition Width: 99.90
[2025-05-20 10:37:56,029][__main__][INFO] - Epoch 6/30
[2025-05-20 10:37:56,029][__main__][INFO] - Train Loss: 0.4143, Train Acc: 85.96%
[2025-05-20 10:37:56,029][__main__][INFO] - Val Loss: 0.3407, Val Acc: 88.04%
[2025-05-20 10:37:56,029][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:37:56,078][__main__][INFO] - Validation loss improved from 0.3417 to 0.3407
[2025-05-20 10:37:56,078][__main__][INFO] - New best model saved with val acc: 88.04%
[2025-05-20 10:42:35,977][__main__][INFO] - Epoch 7 - Breakpoint: 3999.55 Hz, Transition Width: 99.87
[2025-05-20 10:42:35,977][__main__][INFO] - Epoch 7/30
[2025-05-20 10:42:35,978][__main__][INFO] - Train Loss: 0.4049, Train Acc: 85.45%
[2025-05-20 10:42:35,978][__main__][INFO] - Val Loss: 0.3304, Val Acc: 88.41%
[2025-05-20 10:42:35,978][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:42:36,012][__main__][INFO] - Validation loss improved from 0.3314 to 0.3304
[2025-05-20 10:42:36,013][__main__][INFO] - New best model saved with val acc: 88.41%
[2025-05-20 10:47:00,689][__main__][INFO] - Epoch 8 - Breakpoint: 3999.49 Hz, Transition Width: 99.85
[2025-05-20 10:47:00,689][__main__][INFO] - Epoch 8/30
[2025-05-20 10:47:00,689][__main__][INFO] - Train Loss: 0.3744, Train Acc: 86.91%
[2025-05-20 10:47:00,690][__main__][INFO] - Val Loss: 0.3122, Val Acc: 89.67%
[2025-05-20 10:47:00,690][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:47:00,705][__main__][INFO] - Validation loss improved from 0.3132 to 0.3122
[2025-05-20 10:47:00,705][__main__][INFO] - New best model saved with val acc: 89.67%
[2025-05-20 10:51:29,004][__main__][INFO] - Epoch 9 - Breakpoint: 3999.41 Hz, Transition Width: 99.84
[2025-05-20 10:51:29,004][__main__][INFO] - Epoch 9/30
[2025-05-20 10:51:29,004][__main__][INFO] - Train Loss: 0.3660, Train Acc: 87.02%
[2025-05-20 10:51:29,005][__main__][INFO] - Val Loss: 0.3316, Val Acc: 89.42%
[2025-05-20 10:51:29,005][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:51:29,005][__main__][INFO] - No improvement in validation loss for 1/4 epochs
[2025-05-20 10:55:52,237][__main__][INFO] - Epoch 10 - Breakpoint: 3999.34 Hz, Transition Width: 99.82
[2025-05-20 10:55:52,237][__main__][INFO] - Epoch 10/30
[2025-05-20 10:55:52,238][__main__][INFO] - Train Loss: 0.3676, Train Acc: 87.45%
[2025-05-20 10:55:52,238][__main__][INFO] - Val Loss: 0.2992, Val Acc: 89.92%
[2025-05-20 10:55:52,238][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 10:55:52,277][__main__][INFO] - Validation loss improved from 0.3002 to 0.2992
[2025-05-20 10:55:52,277][__main__][INFO] - New best model saved with val acc: 89.92%
[2025-05-20 11:00:14,911][__main__][INFO] - Epoch 11 - Breakpoint: 3999.27 Hz, Transition Width: 99.79
[2025-05-20 11:00:14,911][__main__][INFO] - Epoch 11/30
[2025-05-20 11:00:14,911][__main__][INFO] - Train Loss: 0.3356, Train Acc: 87.55%
[2025-05-20 11:00:14,911][__main__][INFO] - Val Loss: 0.2953, Val Acc: 90.30%
[2025-05-20 11:00:14,911][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 11:00:14,927][__main__][INFO] - Validation loss improved from 0.2963 to 0.2953
[2025-05-20 11:00:14,927][__main__][INFO] - New best model saved with val acc: 90.30%
[2025-05-20 11:04:39,556][__main__][INFO] - Epoch 12 - Breakpoint: 3999.20 Hz, Transition Width: 99.79
[2025-05-20 11:04:39,557][__main__][INFO] - Epoch 12/30
[2025-05-20 11:04:39,557][__main__][INFO] - Train Loss: 0.3474, Train Acc: 87.34%
[2025-05-20 11:04:39,557][__main__][INFO] - Val Loss: 0.3314, Val Acc: 89.29%
[2025-05-20 11:04:39,557][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 11:04:39,557][__main__][INFO] - No improvement in validation loss for 1/4 epochs
[2025-05-20 11:09:17,274][__main__][INFO] - Epoch 13 - Breakpoint: 3999.14 Hz, Transition Width: 99.76
[2025-05-20 11:09:17,275][__main__][INFO] - Epoch 13/30
[2025-05-20 11:09:17,275][__main__][INFO] - Train Loss: 0.3260, Train Acc: 88.39%
[2025-05-20 11:09:17,275][__main__][INFO] - Val Loss: 0.3010, Val Acc: 90.05%
[2025-05-20 11:09:17,275][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 11:09:17,275][__main__][INFO] - No improvement in validation loss for 2/4 epochs
[2025-05-20 11:14:04,597][__main__][INFO] - Epoch 14 - Breakpoint: 3999.08 Hz, Transition Width: 99.73
[2025-05-20 11:14:04,597][__main__][INFO] - Epoch 14/30
[2025-05-20 11:14:04,597][__main__][INFO] - Train Loss: 0.3043, Train Acc: 89.30%
[2025-05-20 11:14:04,597][__main__][INFO] - Val Loss: 0.3025, Val Acc: 90.30%
[2025-05-20 11:14:04,597][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 11:14:04,598][__main__][INFO] - No improvement in validation loss for 3/4 epochs
[2025-05-20 11:18:27,222][__main__][INFO] - Epoch 15 - Breakpoint: 3999.02 Hz, Transition Width: 99.71
[2025-05-20 11:18:27,222][__main__][INFO] - Epoch 15/30
[2025-05-20 11:18:27,222][__main__][INFO] - Train Loss: 0.3170, Train Acc: 88.60%
[2025-05-20 11:18:27,223][__main__][INFO] - Val Loss: 0.3117, Val Acc: 89.55%
[2025-05-20 11:18:27,223][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 11:18:27,223][__main__][INFO] - No improvement in validation loss for 4/4 epochs
[2025-05-20 11:18:27,223][__main__][INFO] - Early stopping triggered.
[2025-05-20 11:18:28,055][__main__][INFO] - Training history plot saved to /raid/home/e3da/interns/lmannini/bird_classification_edge/logs/4birds_CF_combined_log_linear_gru64_30epochs/2025-05-20_10-11-53/training_history.png
[2025-05-20 11:18:28,055][__main__][INFO] - 
Evaluating on test set...
[2025-05-20 11:18:28,101][__main__][INFO] - Loaded best model based on validation loss for testing.
[2025-05-20 11:19:12,079][__main__][INFO] - Test Loss: 0.3092, Test Acc: 89.04%
[2025-05-20 11:19:12,085][__main__][INFO] - Confusion matrix CSV saved to /raid/home/e3da/interns/lmannini/bird_classification_edge/logs/4birds_CF_combined_log_linear_gru64_30epochs/2025-05-20_10-11-53/confusion_matrix.csv
[2025-05-20 11:19:12,243][__main__][INFO] - Confusion matrix PNG saved to /raid/home/e3da/interns/lmannini/bird_classification_edge/logs/4birds_CF_combined_log_linear_gru64_30epochs/2025-05-20_10-11-53/confusion_matrix.png
[2025-05-20 11:19:12,243][__main__][INFO] - Results saved to /raid/home/e3da/interns/lmannini/bird_classification_edge/logs/4birds_CF_combined_log_linear_gru64_30epochs/2025-05-20_10-11-53/results.json
[2025-05-20 11:19:12,244][__main__][INFO] - Model summary saved to /raid/home/e3da/interns/lmannini/bird_classification_edge/logs/4birds_CF_combined_log_linear_gru64_30epochs/2025-05-20_10-11-53/model_summary.txt
[2025-05-20 11:19:12,244][__main__][INFO] - Training completed! Best validation accuracy: 90.30%
[2025-05-20 11:19:12,244][__main__][INFO] - Test accuracy: 89.04%
[2025-05-20 11:19:12,244][__main__][INFO] - Output directory: /raid/home/e3da/interns/lmannini/bird_classification_edge/logs/4birds_CF_combined_log_linear_gru64_30epochs/2025-05-20_10-11-53
