[2025-05-20 07:54:16,835][__main__][INFO] - Experiment: 4birds_CF_combined_log_linear_gru64_30epochs
[2025-05-20 07:54:16,855][__main__][INFO] - Using device: cuda
[2025-05-20 07:54:16,855][__main__][INFO] - Checking for datasets...
[2025-05-20 07:54:16,855][__main__][INFO] - ESC-50 dataset not found or empty. Downloading...
[2025-05-20 07:54:16,856][__main__][INFO] - Creating initial bird sound datasets (train/val/test)...
[2025-05-20 07:54:16,856][__main__][INFO] - Using dataset split parameters: val=0.15, test=0.15, seed=42
[2025-05-20 07:54:16,878][__main__][INFO] - Initial bird samples: Train=2970, Val=635, Test=635
[2025-05-20 07:54:16,878][__main__][INFO] - Calculating target number of 'no birds' samples...
[2025-05-20 07:54:16,878][__main__][INFO] - Number of bird classes: 4. 'No Birds' label index: 4
[2025-05-20 07:54:16,879][__main__][INFO] - Average samples per bird class in training set: 742.50
[2025-05-20 07:54:16,879][__main__][INFO] - Target 'no birds' samples for training: 742
[2025-05-20 07:54:16,879][__main__][INFO] - Target 'no birds' samples for validation: 159
[2025-05-20 07:54:16,879][__main__][INFO] - Target 'no birds' samples for testing: 159
[2025-05-20 07:54:16,893][__main__][INFO] - Combining bird and 'no birds' datasets...
[2025-05-20 07:54:16,893][__main__][INFO] - Final training samples: 3712
[2025-05-20 07:54:16,893][__main__][INFO] - Final validation samples: 794
[2025-05-20 07:54:16,893][__main__][INFO] - Final testing samples: 794
[2025-05-20 07:54:17,061][__main__][INFO] - Model architecture:
[2025-05-20 07:54:17,061][__main__][INFO] - Improved_Phi_GRU_ATT(
  (stft_transform): Spectrogram()
  (amplitude_to_db): AmplitudeToDB()
  (phi): MatchboxNetSkip(
    (initial_conv_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(4, 0), value=0)
            (conv): Conv1d(64, 64, kernel_size=(5,), stride=(1,), groups=64, bias=False)
          )
          (5): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(64, 10, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(10, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (blocks_modulelist): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(8, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(4,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
        (1): Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(4, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), dilation=(2,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
      (2): ModuleList(
        (0-1): 2 x Sequential(
          (0): PhiNetCausalConvBlock(
            (_layers): ModuleList(
              (0): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (2): Hardswish()
              (3): Dropout1d(p=0.05, inplace=False)
              (4): DepthwiseCausalConv(
                (pad): ConstantPad1d(padding=(2, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(3,), stride=(1,), groups=32, bias=False)
              )
              (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
              (6): Hardswish()
              (7): SEBlock(
                (se_conv): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
                )
                (se_conv2): CausalConv1d(
                  (pad): ConstantPad1d(padding=(0, 0), value=0)
                  (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
                )
                (activation): Hardswish()
                (mult): FloatFunctional(
                  (activation_post_process): Identity()
                )
              )
              (8): CausalConv1d(
                (pad): ConstantPad1d(padding=(0, 0), value=0)
                (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
              )
              (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
            )
            (op): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
        )
      )
    )
    (projections_modulelist): ModuleList(
      (0-2): 3 x Identity()
    )
    (final_block_to_conv1_projection): Identity()
    (final_conv1_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(8, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(5,), stride=(1,), dilation=(2,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
    (conv1_to_conv2_projection): Identity()
    (final_conv2_module): Sequential(
      (0): PhiNetCausalConvBlock(
        (_layers): ModuleList(
          (0): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (1): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (2): Hardswish()
          (3): Dropout1d(p=0.05, inplace=False)
          (4): DepthwiseCausalConv(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), groups=32, bias=False)
          )
          (5): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
          (6): Hardswish()
          (7): SEBlock(
            (se_conv): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(32, 5, kernel_size=(1,), stride=(1,), bias=False)
            )
            (se_conv2): CausalConv1d(
              (pad): ConstantPad1d(padding=(0, 0), value=0)
              (conv): Conv1d(5, 32, kernel_size=(1,), stride=(1,), bias=False)
            )
            (activation): Hardswish()
            (mult): FloatFunctional(
              (activation_post_process): Identity()
            )
          )
          (8): CausalConv1d(
            (pad): ConstantPad1d(padding=(0, 0), value=0)
            (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)
          )
          (9): BatchNorm1d(32, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)
        )
      )
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.3, inplace=False)
    )
  )
  (gru): GRU(32, 64, batch_first=True)
  (projection): Linear(in_features=64, out_features=64, bias=True)
  (keyword_attention): AttentionLayer(
    (attention): Linear(in_features=64, out_features=1, bias=True)
  )
  (fc): Linear(in_features=64, out_features=5, bias=True)
)
[2025-05-20 07:54:17,065][__main__][INFO] - Total parameters: 53,254
[2025-05-20 07:54:17,065][__main__][INFO] - Trainable parameters: 53,254
[2025-05-20 07:54:17,065][__main__][INFO] - Computing model complexity...
[2025-05-20 07:54:30,200][__main__][WARNING] - Could not compute MACs: unsupported format string passed to NoneType.__format__
[2025-05-20 07:54:31,049][__main__][INFO] - Starting training...
[2025-05-20 07:59:05,846][__main__][INFO] - Epoch 1/30
[2025-05-20 07:59:05,846][__main__][INFO] - Train Loss: 0.8408, Train Acc: 69.94%
[2025-05-20 07:59:05,846][__main__][INFO] - Val Loss: 0.4856, Val Acc: 84.13%
[2025-05-20 07:59:05,846][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 07:59:05,846][__main__][INFO] - Validation loss improved from inf to 0.4856
[2025-05-20 07:59:05,871][__main__][INFO] - New best model saved with val acc: 84.13%
[2025-05-20 08:03:40,319][__main__][INFO] - Epoch 2/30
[2025-05-20 08:03:40,319][__main__][INFO] - Train Loss: 0.5324, Train Acc: 81.41%
[2025-05-20 08:03:40,320][__main__][INFO] - Val Loss: 0.4555, Val Acc: 84.01%
[2025-05-20 08:03:40,320][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:03:40,320][__main__][INFO] - Validation loss improved from 0.4856 to 0.4555
[2025-05-20 08:08:03,133][__main__][INFO] - Epoch 3/30
[2025-05-20 08:08:03,133][__main__][INFO] - Train Loss: 0.4569, Train Acc: 84.11%
[2025-05-20 08:08:03,133][__main__][INFO] - Val Loss: 0.3857, Val Acc: 87.15%
[2025-05-20 08:08:03,133][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:08:03,134][__main__][INFO] - Validation loss improved from 0.4555 to 0.3857
[2025-05-20 08:08:03,174][__main__][INFO] - New best model saved with val acc: 87.15%
[2025-05-20 08:12:34,282][__main__][INFO] - Epoch 4/30
[2025-05-20 08:12:34,282][__main__][INFO] - Train Loss: 0.4406, Train Acc: 84.64%
[2025-05-20 08:12:34,283][__main__][INFO] - Val Loss: 0.3415, Val Acc: 89.17%
[2025-05-20 08:12:34,283][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:12:34,283][__main__][INFO] - Validation loss improved from 0.3857 to 0.3415
[2025-05-20 08:12:34,321][__main__][INFO] - New best model saved with val acc: 89.17%
[2025-05-20 08:16:55,591][__main__][INFO] - Epoch 5/30
[2025-05-20 08:16:55,591][__main__][INFO] - Train Loss: 0.4026, Train Acc: 85.96%
[2025-05-20 08:16:55,591][__main__][INFO] - Val Loss: 0.3945, Val Acc: 88.04%
[2025-05-20 08:16:55,591][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:16:55,591][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-20 08:21:15,968][__main__][INFO] - Epoch 6/30
[2025-05-20 08:21:15,969][__main__][INFO] - Train Loss: 0.3890, Train Acc: 86.26%
[2025-05-20 08:21:15,969][__main__][INFO] - Val Loss: 0.3345, Val Acc: 88.04%
[2025-05-20 08:21:15,969][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:21:15,969][__main__][INFO] - Validation loss improved from 0.3415 to 0.3345
[2025-05-20 08:25:46,737][__main__][INFO] - Epoch 7/30
[2025-05-20 08:25:46,737][__main__][INFO] - Train Loss: 0.3904, Train Acc: 86.45%
[2025-05-20 08:25:46,737][__main__][INFO] - Val Loss: 0.3566, Val Acc: 87.53%
[2025-05-20 08:25:46,737][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:25:46,737][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-20 08:30:19,301][__main__][INFO] - Epoch 8/30
[2025-05-20 08:30:19,301][__main__][INFO] - Train Loss: 0.3527, Train Acc: 87.80%
[2025-05-20 08:30:19,302][__main__][INFO] - Val Loss: 0.2994, Val Acc: 90.05%
[2025-05-20 08:30:19,302][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:30:19,302][__main__][INFO] - Validation loss improved from 0.3345 to 0.2994
[2025-05-20 08:30:19,356][__main__][INFO] - New best model saved with val acc: 90.05%
[2025-05-20 08:34:55,601][__main__][INFO] - Epoch 9/30
[2025-05-20 08:34:55,601][__main__][INFO] - Train Loss: 0.3224, Train Acc: 88.63%
[2025-05-20 08:34:55,602][__main__][INFO] - Val Loss: 0.3066, Val Acc: 90.43%
[2025-05-20 08:34:55,602][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:34:55,602][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-20 08:34:55,651][__main__][INFO] - New best model saved with val acc: 90.43%
[2025-05-20 08:39:21,869][__main__][INFO] - Epoch 10/30
[2025-05-20 08:39:21,869][__main__][INFO] - Train Loss: 0.3313, Train Acc: 88.04%
[2025-05-20 08:39:21,869][__main__][INFO] - Val Loss: 0.2618, Val Acc: 90.68%
[2025-05-20 08:39:21,869][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:39:21,869][__main__][INFO] - Validation loss improved from 0.2994 to 0.2618
[2025-05-20 08:39:21,894][__main__][INFO] - New best model saved with val acc: 90.68%
[2025-05-20 08:43:47,780][__main__][INFO] - Epoch 11/30
[2025-05-20 08:43:47,780][__main__][INFO] - Train Loss: 0.3169, Train Acc: 88.79%
[2025-05-20 08:43:47,780][__main__][INFO] - Val Loss: 0.3694, Val Acc: 87.91%
[2025-05-20 08:43:47,780][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:43:47,780][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-20 08:48:36,336][__main__][INFO] - Epoch 12/30
[2025-05-20 08:48:36,336][__main__][INFO] - Train Loss: 0.2993, Train Acc: 89.55%
[2025-05-20 08:48:36,336][__main__][INFO] - Val Loss: 0.2791, Val Acc: 90.43%
[2025-05-20 08:48:36,336][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:48:36,336][__main__][INFO] - No improvement in validation loss for 2/10 epochs
[2025-05-20 08:53:11,832][__main__][INFO] - Epoch 13/30
[2025-05-20 08:53:11,832][__main__][INFO] - Train Loss: 0.3109, Train Acc: 88.63%
[2025-05-20 08:53:11,832][__main__][INFO] - Val Loss: 0.2508, Val Acc: 92.19%
[2025-05-20 08:53:11,832][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:53:11,832][__main__][INFO] - Validation loss improved from 0.2618 to 0.2508
[2025-05-20 08:53:11,867][__main__][INFO] - New best model saved with val acc: 92.19%
[2025-05-20 08:57:40,770][__main__][INFO] - Epoch 14/30
[2025-05-20 08:57:40,770][__main__][INFO] - Train Loss: 0.2945, Train Acc: 89.33%
[2025-05-20 08:57:40,770][__main__][INFO] - Val Loss: 0.2347, Val Acc: 92.32%
[2025-05-20 08:57:40,770][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 08:57:40,770][__main__][INFO] - Validation loss improved from 0.2508 to 0.2347
[2025-05-20 08:57:40,796][__main__][INFO] - New best model saved with val acc: 92.32%
[2025-05-20 09:02:17,546][__main__][INFO] - Epoch 15/30
[2025-05-20 09:02:17,546][__main__][INFO] - Train Loss: 0.2854, Train Acc: 89.49%
[2025-05-20 09:02:17,546][__main__][INFO] - Val Loss: 0.2842, Val Acc: 90.55%
[2025-05-20 09:02:17,546][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 09:02:17,546][__main__][INFO] - No improvement in validation loss for 1/10 epochs
[2025-05-20 09:06:53,659][__main__][INFO] - Epoch 16/30
[2025-05-20 09:06:53,659][__main__][INFO] - Train Loss: 0.2801, Train Acc: 89.82%
[2025-05-20 09:06:53,659][__main__][INFO] - Val Loss: 0.2626, Val Acc: 90.93%
[2025-05-20 09:06:53,659][__main__][INFO] - Learning Rate: 0.001000
[2025-05-20 09:06:53,660][__main__][INFO] - No improvement in validation loss for 2/10 epochs
