[2025-06-12 10:24:13,527][__main__][INFO] - Starting knowledge distillation training
[2025-06-12 10:24:13,532][__main__][INFO] - Configuration:
experiment_name: bird_classification_distillation
distillation:
  alpha: 0.3
  temperature: 4.0
  adaptive: false
  adaptation_rate: 0.1
  alpha_schedule: constant
  confidence_threshold: 0.05
soft_labels_path: test_soft_labels
training:
  epochs: 30
  batch_size: 16
  patience: 15
  min_delta: 0.001
  seed: 42
model:
  spectrogram_type: combined_log_linear
  hidden_dim: 64
  n_mel_bins: 64
  n_linear_filters: 64
  trainable_filterbank: true
  initial_breakpoint: 4000.0
  initial_transition_width: 100.0
  n_fft: 1024
  hop_length: 320
  matchbox:
    base_filters: 32
    num_layers: 3
    kernel_size: 3
    dropout: 0.1
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0005
  weight_decay: 0.01
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 5
  min_lr: 1.0e-06
dataset:
  main_data_dir: bird_sound_dataset
  allowed_bird_classes:
  - Poecile montanus
  - Certhia familiaris
  - Apus apus
  - Bubo bubo
  - Periparus ater
  - Emberiza cia
  - Lophophanes cristatus
  - Certhia brachydactyla
  load_pregenerated_no_birds: true
  pregenerated_no_birds_dir: augmented_dataset/no_birds
  num_no_bird_samples: 836
  augmentation:
    enabled: true
    noise_level: 0.01
    time_mask_param: 30
    freq_mask_param: 10
    time_shift_limit: 0.1
    speed_perturb_rate_min: 0.95
    speed_perturb_rate_max: 1.05
  sample_rate: 32000
  clip_duration: 3.0
  lowcut: 150.0
  highcut: 16000.0
  extract_calls: false
  esc50_dir: esc-50/ESC-50-master
  val_split: 0.15
  test_split: 0.15
  seed: 42

[2025-06-12 10:24:13,556][__main__][INFO] - Initialized trainer on device: cpu
[2025-06-12 10:24:13,556][__main__][INFO] - Setting up data loaders with soft labels...
[2025-06-12 10:24:13,662][__main__][INFO] - Train samples: 836
[2025-06-12 10:24:13,662][__main__][INFO] - Val samples: 836
[2025-06-12 10:24:13,662][__main__][INFO] - Test samples: 836
[2025-06-12 10:24:13,662][__main__][INFO] - Soft labels info: {'num_classes': 9, 'target_species': ['Poecile montanus', 'Certhia familiaris', 'Apus apus', 'Bubo bubo', 'Periparus ater', 'Emberiza cia', 'Lophophanes cristatus', 'Certhia brachydactyla', 'non-bird'], 'confidence_threshold': 0.05, 'total_files_with_soft_labels': 9986, 'files_processed': 9986}
[2025-06-12 10:24:13,662][__main__][INFO] - Setting up student model...
[2025-06-12 10:24:13,696][__main__][INFO] - Student model parameters: 53,516
[2025-06-12 10:24:13,696][__main__][INFO] - Setting up optimizer and scheduler...
[2025-06-12 10:24:14,851][__main__][INFO] - Optimizer: AdamW
[2025-06-12 10:24:14,851][__main__][INFO] - Scheduler: ReduceLROnPlateau
[2025-06-12 10:24:14,851][__main__][INFO] - Setting up distillation loss...
[2025-06-12 10:24:14,851][__main__][INFO] - Using Standard DistillationLoss
[2025-06-12 10:24:14,851][__main__][INFO] - Alpha: 0.3, Temperature: 4.0
[2025-06-12 10:24:14,851][__main__][INFO] - Starting distillation training...
[2025-06-12 10:24:15,114][__main__][INFO] - Epoch 0, Batch 0/105, Loss: 1.5196 (Hard: 2.1662, Soft: 0.0108)
[2025-06-12 10:24:23,583][__main__][INFO] - Epoch 0, Batch 50/105, Loss: 0.3805 (Hard: 0.2423, Soft: 0.7029)
[2025-06-12 10:24:33,047][__main__][INFO] - Epoch 0, Batch 100/105, Loss: 0.3791 (Hard: 0.2381, Soft: 0.7082)
[2025-06-12 10:24:39,375][__main__][INFO] - Epoch 0: Train Loss: 0.4957, Train Acc: 97.61%, Val Loss: 0.3801, Val Acc: 100.00%
[2025-06-12 10:24:39,375][__main__][INFO] -   Hard Loss: 0.4340, Soft Loss: 0.6397, Alpha: 0.300
[2025-06-12 10:24:39,413][__main__][INFO] - New best model saved! Val Acc: 100.00%
[2025-06-12 10:24:39,578][__main__][INFO] - Epoch 1, Batch 0/105, Loss: 0.3787 (Hard: 0.2319, Soft: 0.7212)
[2025-06-12 10:24:47,872][__main__][INFO] - Epoch 1, Batch 50/105, Loss: 0.3785 (Hard: 0.2269, Soft: 0.7320)
[2025-06-12 10:24:56,007][__main__][INFO] - Epoch 1, Batch 100/105, Loss: 0.3783 (Hard: 0.2226, Soft: 0.7416)
[2025-06-12 10:25:02,160][__main__][INFO] - Epoch 1: Train Loss: 0.3786, Train Acc: 100.00%, Val Loss: 0.3816, Val Acc: 100.00%
[2025-06-12 10:25:02,160][__main__][INFO] -   Hard Loss: 0.2255, Soft Loss: 0.7358, Alpha: 0.300
[2025-06-12 10:25:02,325][__main__][INFO] - Epoch 2, Batch 0/105, Loss: 0.3784 (Hard: 0.2285, Soft: 0.7282)
[2025-06-12 10:25:10,703][__main__][INFO] - Epoch 2, Batch 50/105, Loss: 0.3786 (Hard: 0.2227, Soft: 0.7425)
[2025-06-12 10:25:19,377][__main__][INFO] - Epoch 2, Batch 100/105, Loss: 0.3785 (Hard: 0.2169, Soft: 0.7555)
[2025-06-12 10:25:24,897][__main__][INFO] - Epoch 2: Train Loss: 0.3785, Train Acc: 100.00%, Val Loss: 0.3834, Val Acc: 100.00%
[2025-06-12 10:25:24,897][__main__][INFO] -   Hard Loss: 0.2251, Soft Loss: 0.7363, Alpha: 0.300
[2025-06-12 10:25:25,062][__main__][INFO] - Epoch 3, Batch 0/105, Loss: 0.3786 (Hard: 0.2223, Soft: 0.7435)
[2025-06-12 10:25:33,245][__main__][INFO] - Epoch 3, Batch 50/105, Loss: 0.3783 (Hard: 0.2248, Soft: 0.7365)
[2025-06-12 10:25:41,479][__main__][INFO] - Epoch 3, Batch 100/105, Loss: 0.3784 (Hard: 0.2211, Soft: 0.7456)
[2025-06-12 10:25:46,992][__main__][INFO] - Epoch 3: Train Loss: 0.3784, Train Acc: 100.00%, Val Loss: 0.3826, Val Acc: 100.00%
[2025-06-12 10:25:46,992][__main__][INFO] -   Hard Loss: 0.2253, Soft Loss: 0.7356, Alpha: 0.300
[2025-06-12 10:25:47,150][__main__][INFO] - Epoch 4, Batch 0/105, Loss: 0.3783 (Hard: 0.2241, Soft: 0.7380)
[2025-06-12 10:25:55,428][__main__][INFO] - Epoch 4, Batch 50/105, Loss: 0.3784 (Hard: 0.2247, Soft: 0.7371)
[2025-06-12 10:26:03,721][__main__][INFO] - Epoch 4, Batch 100/105, Loss: 0.3783 (Hard: 0.2241, Soft: 0.7382)
[2025-06-12 10:26:09,226][__main__][INFO] - Epoch 4: Train Loss: 0.3783, Train Acc: 100.00%, Val Loss: 0.3795, Val Acc: 100.00%
[2025-06-12 10:26:09,226][__main__][INFO] -   Hard Loss: 0.2254, Soft Loss: 0.7352, Alpha: 0.300
[2025-06-12 10:26:09,390][__main__][INFO] - Epoch 5, Batch 0/105, Loss: 0.3783 (Hard: 0.2285, Soft: 0.7278)
[2025-06-12 10:26:17,521][__main__][INFO] - Epoch 5, Batch 50/105, Loss: 0.3783 (Hard: 0.2218, Soft: 0.7435)
[2025-06-12 10:26:25,634][__main__][INFO] - Epoch 5, Batch 100/105, Loss: 0.3783 (Hard: 0.2269, Soft: 0.7316)
[2025-06-12 10:26:31,074][__main__][INFO] - Epoch 5: Train Loss: 0.3783, Train Acc: 100.00%, Val Loss: 0.3811, Val Acc: 100.00%
[2025-06-12 10:26:31,074][__main__][INFO] -   Hard Loss: 0.2253, Soft Loss: 0.7353, Alpha: 0.300
[2025-06-12 10:26:31,239][__main__][INFO] - Epoch 6, Batch 0/105, Loss: 0.3783 (Hard: 0.2262, Soft: 0.7332)
[2025-06-12 10:26:39,442][__main__][INFO] - Epoch 6, Batch 50/105, Loss: 0.3782 (Hard: 0.2260, Soft: 0.7335)
[2025-06-12 10:26:47,612][__main__][INFO] - Epoch 6, Batch 100/105, Loss: 0.3782 (Hard: 0.2270, Soft: 0.7311)
[2025-06-12 10:26:53,225][__main__][INFO] - Epoch 6: Train Loss: 0.3783, Train Acc: 100.00%, Val Loss: 0.3818, Val Acc: 100.00%
[2025-06-12 10:26:53,226][__main__][INFO] -   Hard Loss: 0.2252, Soft Loss: 0.7354, Alpha: 0.300
[2025-06-12 10:26:53,393][__main__][INFO] - Epoch 7, Batch 0/105, Loss: 0.3783 (Hard: 0.2277, Soft: 0.7298)
[2025-06-12 10:27:01,905][__main__][INFO] - Epoch 7, Batch 50/105, Loss: 0.3783 (Hard: 0.2170, Soft: 0.7548)
[2025-06-12 10:27:09,982][__main__][INFO] - Epoch 7, Batch 100/105, Loss: 0.3782 (Hard: 0.2212, Soft: 0.7446)
[2025-06-12 10:27:15,620][__main__][INFO] - Epoch 7: Train Loss: 0.3783, Train Acc: 100.00%, Val Loss: 0.3833, Val Acc: 100.00%
[2025-06-12 10:27:15,620][__main__][INFO] -   Hard Loss: 0.2253, Soft Loss: 0.7353, Alpha: 0.300
[2025-06-12 10:27:15,781][__main__][INFO] - Epoch 8, Batch 0/105, Loss: 0.3782 (Hard: 0.2285, Soft: 0.7277)
[2025-06-12 10:27:23,814][__main__][INFO] - Epoch 8, Batch 50/105, Loss: 0.3782 (Hard: 0.2232, Soft: 0.7400)
[2025-06-12 10:27:31,936][__main__][INFO] - Epoch 8, Batch 100/105, Loss: 0.3782 (Hard: 0.2199, Soft: 0.7477)
[2025-06-12 10:27:37,529][__main__][INFO] - Epoch 8: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3791, Val Acc: 100.00%
[2025-06-12 10:27:37,529][__main__][INFO] -   Hard Loss: 0.2252, Soft Loss: 0.7352, Alpha: 0.300
[2025-06-12 10:27:37,693][__main__][INFO] - Epoch 9, Batch 0/105, Loss: 0.3782 (Hard: 0.2258, Soft: 0.7338)
[2025-06-12 10:27:45,788][__main__][INFO] - Epoch 9, Batch 50/105, Loss: 0.3783 (Hard: 0.2259, Soft: 0.7337)
[2025-06-12 10:27:54,043][__main__][INFO] - Epoch 9, Batch 100/105, Loss: 0.3783 (Hard: 0.2327, Soft: 0.7181)
[2025-06-12 10:27:59,809][__main__][INFO] - Epoch 9: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3820, Val Acc: 100.00%
[2025-06-12 10:27:59,809][__main__][INFO] -   Hard Loss: 0.2254, Soft Loss: 0.7348, Alpha: 0.300
[2025-06-12 10:27:59,970][__main__][INFO] - Epoch 10, Batch 0/105, Loss: 0.3782 (Hard: 0.2219, Soft: 0.7431)
[2025-06-12 10:28:08,186][__main__][INFO] - Epoch 10, Batch 50/105, Loss: 0.3782 (Hard: 0.2209, Soft: 0.7452)
[2025-06-12 10:28:16,280][__main__][INFO] - Epoch 10, Batch 100/105, Loss: 0.3783 (Hard: 0.2190, Soft: 0.7498)
[2025-06-12 10:28:21,740][__main__][INFO] - Epoch 10: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3789, Val Acc: 100.00%
[2025-06-12 10:28:21,740][__main__][INFO] -   Hard Loss: 0.2251, Soft Loss: 0.7356, Alpha: 0.300
[2025-06-12 10:28:21,910][__main__][INFO] - Epoch 11, Batch 0/105, Loss: 0.3782 (Hard: 0.2245, Soft: 0.7367)
[2025-06-12 10:28:30,210][__main__][INFO] - Epoch 11, Batch 50/105, Loss: 0.3782 (Hard: 0.2249, Soft: 0.7360)
[2025-06-12 10:28:38,652][__main__][INFO] - Epoch 11, Batch 100/105, Loss: 0.3782 (Hard: 0.2264, Soft: 0.7323)
[2025-06-12 10:28:44,280][__main__][INFO] - Epoch 11: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3819, Val Acc: 100.00%
[2025-06-12 10:28:44,280][__main__][INFO] -   Hard Loss: 0.2253, Soft Loss: 0.7350, Alpha: 0.300
[2025-06-12 10:28:44,444][__main__][INFO] - Epoch 12, Batch 0/105, Loss: 0.3782 (Hard: 0.2229, Soft: 0.7405)
[2025-06-12 10:28:52,693][__main__][INFO] - Epoch 12, Batch 50/105, Loss: 0.3782 (Hard: 0.2230, Soft: 0.7405)
[2025-06-12 10:29:00,904][__main__][INFO] - Epoch 12, Batch 100/105, Loss: 0.3782 (Hard: 0.2268, Soft: 0.7315)
[2025-06-12 10:29:06,588][__main__][INFO] - Epoch 12: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3809, Val Acc: 100.00%
[2025-06-12 10:29:06,588][__main__][INFO] -   Hard Loss: 0.2251, Soft Loss: 0.7353, Alpha: 0.300
[2025-06-12 10:29:06,754][__main__][INFO] - Epoch 13, Batch 0/105, Loss: 0.3782 (Hard: 0.2257, Soft: 0.7339)
[2025-06-12 10:29:14,805][__main__][INFO] - Epoch 13, Batch 50/105, Loss: 0.3782 (Hard: 0.2228, Soft: 0.7407)
[2025-06-12 10:29:22,832][__main__][INFO] - Epoch 13, Batch 100/105, Loss: 0.3782 (Hard: 0.2264, Soft: 0.7322)
[2025-06-12 10:29:28,269][__main__][INFO] - Epoch 13: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3795, Val Acc: 100.00%
[2025-06-12 10:29:28,269][__main__][INFO] -   Hard Loss: 0.2253, Soft Loss: 0.7350, Alpha: 0.300
[2025-06-12 10:29:28,438][__main__][INFO] - Epoch 14, Batch 0/105, Loss: 0.3782 (Hard: 0.2249, Soft: 0.7358)
[2025-06-12 10:29:36,754][__main__][INFO] - Epoch 14, Batch 50/105, Loss: 0.3782 (Hard: 0.2279, Soft: 0.7287)
[2025-06-12 10:29:44,912][__main__][INFO] - Epoch 14, Batch 100/105, Loss: 0.3782 (Hard: 0.2302, Soft: 0.7235)
[2025-06-12 10:29:50,315][__main__][INFO] - Epoch 14: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3794, Val Acc: 100.00%
[2025-06-12 10:29:50,315][__main__][INFO] -   Hard Loss: 0.2253, Soft Loss: 0.7350, Alpha: 0.300
[2025-06-12 10:29:50,475][__main__][INFO] - Epoch 15, Batch 0/105, Loss: 0.3781 (Hard: 0.2262, Soft: 0.7327)
[2025-06-12 10:29:58,540][__main__][INFO] - Epoch 15, Batch 50/105, Loss: 0.3781 (Hard: 0.2231, Soft: 0.7400)
[2025-06-12 10:30:06,621][__main__][INFO] - Epoch 15, Batch 100/105, Loss: 0.3782 (Hard: 0.2269, Soft: 0.7311)
[2025-06-12 10:30:12,153][__main__][INFO] - Epoch 15: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3812, Val Acc: 100.00%
[2025-06-12 10:30:12,153][__main__][INFO] -   Hard Loss: 0.2252, Soft Loss: 0.7351, Alpha: 0.300
[2025-06-12 10:30:12,316][__main__][INFO] - Epoch 16, Batch 0/105, Loss: 0.3782 (Hard: 0.2220, Soft: 0.7425)
[2025-06-12 10:30:20,596][__main__][INFO] - Epoch 16, Batch 50/105, Loss: 0.3782 (Hard: 0.2295, Soft: 0.7252)
[2025-06-12 10:30:28,841][__main__][INFO] - Epoch 16, Batch 100/105, Loss: 0.3781 (Hard: 0.2266, Soft: 0.7317)
[2025-06-12 10:30:34,268][__main__][INFO] - Epoch 16: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3800, Val Acc: 100.00%
[2025-06-12 10:30:34,268][__main__][INFO] -   Hard Loss: 0.2251, Soft Loss: 0.7352, Alpha: 0.300
[2025-06-12 10:30:34,425][__main__][INFO] - Epoch 17, Batch 0/105, Loss: 0.3782 (Hard: 0.2229, Soft: 0.7405)
[2025-06-12 10:30:42,504][__main__][INFO] - Epoch 17, Batch 50/105, Loss: 0.3782 (Hard: 0.2260, Soft: 0.7332)
[2025-06-12 10:30:50,593][__main__][INFO] - Epoch 17, Batch 100/105, Loss: 0.3781 (Hard: 0.2264, Soft: 0.7323)
[2025-06-12 10:30:56,081][__main__][INFO] - Epoch 17: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3793, Val Acc: 100.00%
[2025-06-12 10:30:56,081][__main__][INFO] -   Hard Loss: 0.2252, Soft Loss: 0.7351, Alpha: 0.300
[2025-06-12 10:30:56,246][__main__][INFO] - Epoch 18, Batch 0/105, Loss: 0.3781 (Hard: 0.2257, Soft: 0.7337)
[2025-06-12 10:31:04,515][__main__][INFO] - Epoch 18, Batch 50/105, Loss: 0.3781 (Hard: 0.2260, Soft: 0.7331)
[2025-06-12 10:31:12,889][__main__][INFO] - Epoch 18, Batch 100/105, Loss: 0.3781 (Hard: 0.2245, Soft: 0.7367)
[2025-06-12 10:31:18,477][__main__][INFO] - Epoch 18: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3832, Val Acc: 100.00%
[2025-06-12 10:31:18,478][__main__][INFO] -   Hard Loss: 0.2253, Soft Loss: 0.7348, Alpha: 0.300
[2025-06-12 10:31:18,641][__main__][INFO] - Epoch 19, Batch 0/105, Loss: 0.3782 (Hard: 0.2264, Soft: 0.7323)
[2025-06-12 10:31:26,949][__main__][INFO] - Epoch 19, Batch 50/105, Loss: 0.3782 (Hard: 0.2226, Soft: 0.7412)
[2025-06-12 10:31:35,268][__main__][INFO] - Epoch 19, Batch 100/105, Loss: 0.3782 (Hard: 0.2244, Soft: 0.7368)
[2025-06-12 10:31:40,954][__main__][INFO] - Epoch 19: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3789, Val Acc: 100.00%
[2025-06-12 10:31:40,954][__main__][INFO] -   Hard Loss: 0.2251, Soft Loss: 0.7353, Alpha: 0.300
[2025-06-12 10:31:41,127][__main__][INFO] - Epoch 20, Batch 0/105, Loss: 0.3781 (Hard: 0.2260, Soft: 0.7331)
[2025-06-12 10:31:49,373][__main__][INFO] - Epoch 20, Batch 50/105, Loss: 0.3782 (Hard: 0.2238, Soft: 0.7384)
[2025-06-12 10:31:57,596][__main__][INFO] - Epoch 20, Batch 100/105, Loss: 0.3782 (Hard: 0.2273, Soft: 0.7301)
[2025-06-12 10:32:02,952][__main__][INFO] - Epoch 20: Train Loss: 0.3782, Train Acc: 100.00%, Val Loss: 0.3809, Val Acc: 100.00%
[2025-06-12 10:32:02,952][__main__][INFO] -   Hard Loss: 0.2253, Soft Loss: 0.7349, Alpha: 0.300
[2025-06-12 10:32:03,117][__main__][INFO] - Epoch 21, Batch 0/105, Loss: 0.3782 (Hard: 0.2239, Soft: 0.7381)
[2025-06-12 10:32:11,150][__main__][INFO] - Epoch 21, Batch 50/105, Loss: 0.3781 (Hard: 0.2256, Soft: 0.7340)
[2025-06-12 10:32:19,188][__main__][INFO] - Epoch 21, Batch 100/105, Loss: 0.3781 (Hard: 0.2265, Soft: 0.7320)
[2025-06-12 10:32:24,695][__main__][INFO] - Epoch 21: Train Loss: 0.3781, Train Acc: 100.00%, Val Loss: 0.3791, Val Acc: 100.00%
[2025-06-12 10:32:24,695][__main__][INFO] -   Hard Loss: 0.2252, Soft Loss: 0.7351, Alpha: 0.300
[2025-06-12 10:32:24,858][__main__][INFO] - Epoch 22, Batch 0/105, Loss: 0.3781 (Hard: 0.2243, Soft: 0.7372)
[2025-06-12 10:32:33,015][__main__][INFO] - Epoch 22, Batch 50/105, Loss: 0.3782 (Hard: 0.2229, Soft: 0.7405)
[2025-06-12 10:32:41,016][__main__][INFO] - Epoch 22, Batch 100/105, Loss: 0.3781 (Hard: 0.2255, Soft: 0.7342)
[2025-06-12 10:32:46,383][__main__][INFO] - Epoch 22: Train Loss: 0.3781, Train Acc: 100.00%, Val Loss: 0.3792, Val Acc: 100.00%
[2025-06-12 10:32:46,383][__main__][INFO] -   Hard Loss: 0.2252, Soft Loss: 0.7351, Alpha: 0.300
[2025-06-12 10:32:46,540][__main__][INFO] - Epoch 23, Batch 0/105, Loss: 0.3781 (Hard: 0.2252, Soft: 0.7350)
[2025-06-12 10:32:54,726][__main__][INFO] - Epoch 23, Batch 50/105, Loss: 0.3781 (Hard: 0.2264, Soft: 0.7322)
[2025-06-12 10:33:02,997][__main__][INFO] - Epoch 23, Batch 100/105, Loss: 0.3782 (Hard: 0.2275, Soft: 0.7296)
[2025-06-12 10:33:08,728][__main__][INFO] - Epoch 23: Train Loss: 0.3781, Train Acc: 100.00%, Val Loss: 0.3790, Val Acc: 100.00%
[2025-06-12 10:33:08,729][__main__][INFO] -   Hard Loss: 0.2252, Soft Loss: 0.7350, Alpha: 0.300
[2025-06-12 10:33:08,897][__main__][INFO] - Epoch 24, Batch 0/105, Loss: 0.3781 (Hard: 0.2249, Soft: 0.7357)
[2025-06-12 10:33:17,082][__main__][INFO] - Epoch 24, Batch 50/105, Loss: 0.3781 (Hard: 0.2250, Soft: 0.7355)
[2025-06-12 10:33:25,334][__main__][INFO] - Epoch 24, Batch 100/105, Loss: 0.3781 (Hard: 0.2245, Soft: 0.7367)
[2025-06-12 10:33:30,724][__main__][INFO] - Epoch 24: Train Loss: 0.3781, Train Acc: 100.00%, Val Loss: 0.3809, Val Acc: 100.00%
[2025-06-12 10:33:30,725][__main__][INFO] -   Hard Loss: 0.2252, Soft Loss: 0.7350, Alpha: 0.300
[2025-06-12 10:33:30,889][__main__][INFO] - Epoch 25, Batch 0/105, Loss: 0.3781 (Hard: 0.2251, Soft: 0.7353)
[2025-06-12 10:33:38,941][__main__][INFO] - Epoch 25, Batch 50/105, Loss: 0.3782 (Hard: 0.2269, Soft: 0.7312)
[2025-06-12 10:33:47,173][__main__][INFO] - Epoch 25, Batch 100/105, Loss: 0.3781 (Hard: 0.2273, Soft: 0.7301)
[2025-06-12 10:33:52,767][__main__][INFO] - Epoch 25: Train Loss: 0.3781, Train Acc: 100.00%, Val Loss: 0.3790, Val Acc: 100.00%
[2025-06-12 10:33:52,767][__main__][INFO] -   Hard Loss: 0.2253, Soft Loss: 0.7349, Alpha: 0.300
[2025-06-12 10:33:52,767][__main__][INFO] - Early stopping triggered at epoch 25
[2025-06-12 10:33:52,767][__main__][INFO] - Training completed!
[2025-06-12 10:33:52,767][__main__][INFO] - Best validation accuracy: 100.00%
[2025-06-12 10:33:52,767][__main__][INFO] - Testing best model...
[2025-06-12 10:33:57,828][__main__][INFO] - Test Accuracy: 100.00%
[2025-06-12 10:33:57,839][__main__][INFO] - Classification Report:
                       precision    recall  f1-score   support

     Poecile montanus       0.00      0.00      0.00         0
   Certhia familiaris       0.00      0.00      0.00         0
            Apus apus       0.00      0.00      0.00         0
            Bubo bubo       0.00      0.00      0.00         0
       Periparus ater       0.00      0.00      0.00         0
         Emberiza cia       0.00      0.00      0.00         0
Lophophanes cristatus       0.00      0.00      0.00         0
Certhia brachydactyla       0.00      0.00      0.00         0
             non-bird       1.00      1.00      1.00       836

             accuracy                           1.00       836
            macro avg       0.11      0.11      0.11       836
         weighted avg       1.00      1.00      1.00       836

[2025-06-12 10:33:59,847][__main__][INFO] - Training plots saved to distillation_training_history.png
[2025-06-12 10:33:59,848][__main__][INFO] - Distillation training completed successfully!
[2025-06-12 10:33:59,848][__main__][INFO] - Final test accuracy: 100.00%
