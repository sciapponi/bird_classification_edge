[2025-06-12 14:21:41,506][__main__][INFO] - Starting knowledge distillation training
[2025-06-12 14:21:41,512][__main__][INFO] - Configuration:
experiment_name: bird_classification_distillation
distillation:
  alpha: 0.3
  temperature: 4.0
  adaptive: false
  adaptation_rate: 0.1
  alpha_schedule: constant
  confidence_threshold: 0.05
soft_labels_path: test_soft_labels
training:
  epochs: 10
  batch_size: 16
  patience: 15
  min_delta: 0.001
  seed: 42
model:
  spectrogram_type: combined_log_linear
  hidden_dim: 64
  n_mel_bins: 64
  n_linear_filters: 64
  trainable_filterbank: true
  initial_breakpoint: 4000.0
  initial_transition_width: 100.0
  n_fft: 1024
  hop_length: 320
  matchbox:
    base_filters: 32
    num_layers: 3
    kernel_size: 3
    dropout: 0.1
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0005
  weight_decay: 0.01
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: min
  factor: 0.5
  patience: 5
  min_lr: 1.0e-06
dataset:
  main_data_dir: bird_sound_dataset
  allowed_bird_classes:
  - Bubo_bubo
  - Certhia_familiaris
  - Apus_apus
  - Certhia_brachydactyla
  - Emberiza_cia
  - Lophophanes_cristatus
  - Periparus_ater
  - Poecile_montanus
  load_pregenerated_no_birds: true
  pregenerated_no_birds_dir: augmented_dataset/no_birds
  num_no_bird_samples: 836
  augmentation:
    enabled: true
    noise_level: 0.01
    time_mask_param: 30
    freq_mask_param: 10
    time_shift_limit: 0.1
    speed_perturb_rate_min: 0.95
    speed_perturb_rate_max: 1.05
  sample_rate: 32000
  clip_duration: 3.0
  lowcut: 150.0
  highcut: 16000.0
  extract_calls: false
  esc50_dir: esc-50/ESC-50-master
  val_split: 0.15
  test_split: 0.15
  seed: 42

[2025-06-12 14:21:41,517][__main__][INFO] - Initialized trainer on device: cpu
[2025-06-12 14:21:41,517][__main__][INFO] - Setting up data loaders with soft labels...
[2025-06-12 14:21:41,698][__main__][INFO] - Train samples: 7832
[2025-06-12 14:21:41,698][__main__][INFO] - Val samples: 2331
[2025-06-12 14:21:41,698][__main__][INFO] - Test samples: 2331
[2025-06-12 14:21:41,698][__main__][INFO] - Soft labels info: {'num_classes': 9, 'target_species': ['Poecile montanus', 'Certhia familiaris', 'Apus apus', 'Bubo bubo', 'Periparus ater', 'Emberiza cia', 'Lophophanes cristatus', 'Certhia brachydactyla', 'non-bird'], 'confidence_threshold': 0.05, 'total_files_with_soft_labels': 9986, 'files_processed': 9986}
[2025-06-12 14:21:41,698][__main__][INFO] - Setting up student model...
[2025-06-12 14:21:41,716][__main__][INFO] - Student model parameters: 53,516
[2025-06-12 14:21:41,717][__main__][INFO] - Setting up optimizer and scheduler...
[2025-06-12 14:21:42,877][__main__][INFO] - Optimizer: AdamW
[2025-06-12 14:21:42,877][__main__][INFO] - Scheduler: ReduceLROnPlateau
[2025-06-12 14:21:42,877][__main__][INFO] - Setting up distillation loss...
[2025-06-12 14:21:42,877][__main__][INFO] - Using Standard DistillationLoss
[2025-06-12 14:21:42,877][__main__][INFO] - Alpha: 0.3, Temperature: 4.0
[2025-06-12 14:21:42,877][__main__][INFO] - Starting distillation training...
[2025-06-12 14:21:53,413][__main__][INFO] - Epoch 0, Batch 0/979, Loss: 1.5708 (Hard: 2.2250, Soft: 0.0443)
[2025-06-12 14:23:18,637][__main__][INFO] - Epoch 0, Batch 50/979, Loss: 1.5848 (Hard: 2.2246, Soft: 0.0919)
[2025-06-12 14:24:48,060][__main__][INFO] - Epoch 0, Batch 100/979, Loss: 1.4198 (Hard: 1.9773, Soft: 0.1191)
[2025-06-12 14:26:25,048][__main__][INFO] - Epoch 0, Batch 150/979, Loss: 1.4616 (Hard: 1.9811, Soft: 0.2494)
[2025-06-12 14:27:51,948][__main__][INFO] - Epoch 0, Batch 200/979, Loss: 1.1198 (Hard: 1.4503, Soft: 0.3486)
[2025-06-12 14:29:27,179][__main__][INFO] - Epoch 0, Batch 250/979, Loss: 1.4505 (Hard: 1.9892, Soft: 0.1936)
[2025-06-12 14:31:00,960][__main__][INFO] - Epoch 0, Batch 300/979, Loss: 1.3710 (Hard: 1.8289, Soft: 0.3026)
[2025-06-12 14:32:25,813][__main__][INFO] - Epoch 0, Batch 350/979, Loss: 1.2441 (Hard: 1.6308, Soft: 0.3419)
[2025-06-12 14:34:02,076][__main__][INFO] - Epoch 0, Batch 400/979, Loss: 1.2973 (Hard: 1.7252, Soft: 0.2989)
[2025-06-12 14:35:40,211][__main__][INFO] - Epoch 0, Batch 450/979, Loss: 1.0905 (Hard: 1.3870, Soft: 0.3989)
[2025-06-12 14:37:22,878][__main__][INFO] - Epoch 0, Batch 500/979, Loss: 1.0027 (Hard: 1.2872, Soft: 0.3388)
[2025-06-12 14:38:56,821][__main__][INFO] - Epoch 0, Batch 550/979, Loss: 1.0451 (Hard: 1.3357, Soft: 0.3670)
[2025-06-12 14:40:23,681][__main__][INFO] - Epoch 0, Batch 600/979, Loss: 0.9564 (Hard: 1.1694, Soft: 0.4595)
[2025-06-12 14:41:56,716][__main__][INFO] - Epoch 0, Batch 650/979, Loss: 1.0204 (Hard: 1.2870, Soft: 0.3984)
[2025-06-12 14:43:22,380][__main__][INFO] - Epoch 0, Batch 700/979, Loss: 1.1503 (Hard: 1.5166, Soft: 0.2955)
[2025-06-12 14:45:04,197][__main__][INFO] - Epoch 0, Batch 750/979, Loss: 0.9233 (Hard: 1.1108, Soft: 0.4859)
[2025-06-12 14:46:41,521][__main__][INFO] - Epoch 0, Batch 800/979, Loss: 0.8873 (Hard: 1.0728, Soft: 0.4545)
[2025-06-12 14:48:18,011][__main__][INFO] - Epoch 0, Batch 850/979, Loss: 0.7886 (Hard: 0.9523, Soft: 0.4064)
[2025-06-12 14:49:41,032][__main__][INFO] - Epoch 0, Batch 900/979, Loss: 0.7807 (Hard: 0.8870, Soft: 0.5326)
[2025-06-12 14:51:11,410][__main__][INFO] - Epoch 0, Batch 950/979, Loss: 1.2570 (Hard: 1.6301, Soft: 0.3865)
[2025-06-12 14:57:08,512][__main__][INFO] - Epoch 0: Train Loss: 1.1566, Train Acc: 50.97%, Val Loss: 0.8711, Val Acc: 70.87%
[2025-06-12 14:57:08,512][__main__][INFO] -   Hard Loss: 1.5115, Soft Loss: 0.3286, Alpha: 0.300
[2025-06-12 14:57:08,548][__main__][INFO] - New best model saved! Val Acc: 70.87%
[2025-06-12 14:57:11,590][__main__][INFO] - Epoch 1, Batch 0/979, Loss: 1.6215 (Hard: 2.1693, Soft: 0.3432)
[2025-06-12 14:58:46,001][__main__][INFO] - Epoch 1, Batch 50/979, Loss: 1.4594 (Hard: 1.8667, Soft: 0.5090)
[2025-06-12 15:00:22,969][__main__][INFO] - Epoch 1, Batch 100/979, Loss: 0.8452 (Hard: 1.0467, Soft: 0.3751)
[2025-06-12 15:01:57,763][__main__][INFO] - Epoch 1, Batch 150/979, Loss: 1.0391 (Hard: 1.3022, Soft: 0.4251)
[2025-06-12 15:03:50,046][__main__][INFO] - Epoch 1, Batch 200/979, Loss: 0.8241 (Hard: 0.9961, Soft: 0.4227)
[2025-06-12 15:05:21,911][__main__][INFO] - Epoch 1, Batch 250/979, Loss: 1.2351 (Hard: 1.5797, Soft: 0.4311)
[2025-06-12 15:06:59,524][__main__][INFO] - Epoch 1, Batch 300/979, Loss: 0.7793 (Hard: 0.9334, Soft: 0.4197)
[2025-06-12 15:08:32,071][__main__][INFO] - Epoch 1, Batch 350/979, Loss: 1.3151 (Hard: 1.7723, Soft: 0.2482)
[2025-06-12 15:10:04,914][__main__][INFO] - Epoch 1, Batch 400/979, Loss: 1.1283 (Hard: 1.4543, Soft: 0.3678)
[2025-06-12 15:11:38,169][__main__][INFO] - Epoch 1, Batch 450/979, Loss: 0.9144 (Hard: 1.1074, Soft: 0.4640)
[2025-06-12 15:12:59,101][__main__][INFO] - Epoch 1, Batch 500/979, Loss: 0.9732 (Hard: 1.1860, Soft: 0.4765)
[2025-06-12 15:14:26,309][__main__][INFO] - Epoch 1, Batch 550/979, Loss: 0.8540 (Hard: 1.0292, Soft: 0.4452)
[2025-06-12 15:16:10,999][__main__][INFO] - Epoch 1, Batch 600/979, Loss: 1.0977 (Hard: 1.4111, Soft: 0.3664)
[2025-06-12 15:17:48,331][__main__][INFO] - Epoch 1, Batch 650/979, Loss: 0.7591 (Hard: 0.9111, Soft: 0.4047)
[2025-06-12 15:19:22,613][__main__][INFO] - Epoch 1, Batch 700/979, Loss: 1.0762 (Hard: 1.3958, Soft: 0.3305)
[2025-06-12 15:20:48,092][__main__][INFO] - Epoch 1, Batch 750/979, Loss: 0.9250 (Hard: 1.1673, Soft: 0.3595)
[2025-06-12 15:22:16,636][__main__][INFO] - Epoch 1, Batch 800/979, Loss: 0.9618 (Hard: 1.1915, Soft: 0.4259)
[2025-06-12 15:23:39,446][__main__][INFO] - Epoch 1, Batch 850/979, Loss: 1.3839 (Hard: 1.7701, Soft: 0.4829)
[2025-06-12 15:25:08,191][__main__][INFO] - Epoch 1, Batch 900/979, Loss: 0.7616 (Hard: 0.8658, Soft: 0.5184)
[2025-06-12 15:26:38,058][__main__][INFO] - Epoch 1, Batch 950/979, Loss: 1.0429 (Hard: 1.2898, Soft: 0.4669)
[2025-06-12 15:32:43,840][__main__][INFO] - Epoch 1: Train Loss: 0.9850, Train Acc: 62.72%, Val Loss: 0.8092, Val Acc: 75.46%
[2025-06-12 15:32:43,840][__main__][INFO] -   Hard Loss: 1.2181, Soft Loss: 0.4413, Alpha: 0.300
[2025-06-12 15:32:43,874][__main__][INFO] - New best model saved! Val Acc: 75.46%
[2025-06-12 15:32:44,802][__main__][INFO] - Epoch 2, Batch 0/979, Loss: 0.7000 (Hard: 0.7590, Soft: 0.5623)
[2025-06-12 15:34:24,704][__main__][INFO] - Epoch 2, Batch 50/979, Loss: 1.1647 (Hard: 1.4621, Soft: 0.4708)
[2025-06-12 15:36:01,852][__main__][INFO] - Epoch 2, Batch 100/979, Loss: 0.9881 (Hard: 1.2679, Soft: 0.3353)
[2025-06-12 15:37:32,218][__main__][INFO] - Epoch 2, Batch 150/979, Loss: 0.8511 (Hard: 1.0538, Soft: 0.3783)
[2025-06-12 15:39:06,040][__main__][INFO] - Epoch 2, Batch 200/979, Loss: 0.6903 (Hard: 0.7551, Soft: 0.5390)
[2025-06-12 15:40:34,491][__main__][INFO] - Epoch 2, Batch 250/979, Loss: 1.2840 (Hard: 1.6091, Soft: 0.5254)
[2025-06-12 15:42:10,447][__main__][INFO] - Epoch 2, Batch 300/979, Loss: 0.8052 (Hard: 0.9514, Soft: 0.4639)
[2025-06-12 15:43:39,618][__main__][INFO] - Epoch 2, Batch 350/979, Loss: 0.8949 (Hard: 1.0938, Soft: 0.4309)
[2025-06-12 15:45:13,015][__main__][INFO] - Epoch 2, Batch 400/979, Loss: 0.7870 (Hard: 0.8339, Soft: 0.6775)
